{
  "hash": "06fd72b055c30f66888ec36a208d3bdb",
  "result": {
    "markdown": "---\ntitle: \"Human Computer Interaction Experiments\"\n---\n\n\nFollowing are my course notes from a 2018 Coursera course called *Designing, Running, and Analyzing Experiments*, taught by Jacob Wobbrock, a prominent HCI scholar. Note that Wobbrock is in no way responsible for any errors or deviations from his presentation.\n\nThese course notes are an example of *reproducible research* and *literate programming*. They are reproducible research because the same file that generated this html document also ran all the experiments. This is an example of literate programming in the sense that the code, pictures, equations, and narrative are all encapsulated in one file. The source file for this project, along with the data files, are enough for you to reproduce the results and reproduce the documentation. All the source material is available in my github account, although in an obscure location therein.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(readr.show_col_types=FALSE) # supress column type messages\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggthemes)\n```\n:::\n## How many prefer this over that? (Tests of proportions)\n\n### How many prefer website A over B? (One sample test of proportions in two categories)\n\nSixty subjects were asked whether they preferred website A or B. Their answer and a subject ID were recorded. Read the data and describe it.\n\n::: {.cell}\n\n```{.r .cell-code}\nprefsAB <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsAB.csv\"))\ntail(prefsAB) # displays the last few rows of the data frame\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  Subject Pref \n    <dbl> <chr>\n1      55 A    \n2      56 B    \n3      57 A    \n4      58 B    \n5      59 B    \n6      60 A    \n```\n:::\n\n```{.r .cell-code}\nprefsAB$Subject <- factor(prefsAB$Subject) # convert to nominal factor\nprefsAB$Pref <- factor(prefsAB$Pref) # convert to nominal factor\nsummary(prefsAB)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject   Pref  \n 1      : 1   A:14  \n 2      : 1   B:46  \n 3      : 1         \n 4      : 1         \n 5      : 1         \n 6      : 1         \n (Other):54         \n```\n:::\n\n```{.r .cell-code}\nggplot(prefsAB,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\nIs the difference between preferences significant? A default $\\chi^2$ test examines the proportions in two bins, expecting them to be equally apportioned.\n\nTo do the $\\chi^2$ test, first crosstabulate the data with `xtabs()`.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. Pearson chi-square test\nprfs <- xtabs( ~ Pref, data=prefsAB)\nprfs # show counts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPref\n A  B \n14 46 \n```\n:::\n\n```{.r .cell-code}\nchisq.test(prfs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tChi-squared test for given probabilities\n\ndata:  prfs\nX-squared = 17.067, df = 1, p-value = 3.609e-05\n```\n:::\n:::\n\nWe don't really need an exact binomial test yet because the $\\chi^2$ test told us enough: that the difference is not likely due to chance. That was only because there are only two choices. If there were more than two, we'd need a binomial test for every pair if the $\\chi^2$ test turned up a significant difference. This binomial test just foreshadows what we'll need when we face three categories.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. binomial test\n#. binom.test(prfs,split.table=Inf)\nbinom.test(prfs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact binomial test\n\ndata:  prfs\nnumber of successes = 14, number of trials = 60, p-value = 4.224e-05\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1338373 0.3603828\nsample estimates:\nprobability of success \n             0.2333333 \n```\n:::\n:::\n\n### How many prefer website A, B, or C? (One sample test of proportions in three categories)\n\nFirst, read in and describe the data.\nConvert Subject to a factor because R reads any numerical data as, well, numeric, but we don't want to treat it as such. R interprets any data with characters as a factor. We want Subject to be treated as a factor.\n\n::: {.cell}\n\n```{.r .cell-code}\nprefsABC <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABC.csv\"))\nhead(prefsABC) # displays the first few rows of the data frame\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  Subject Pref \n    <dbl> <chr>\n1       1 C    \n2       2 C    \n3       3 B    \n4       4 C    \n5       5 C    \n6       6 B    \n```\n:::\n\n```{.r .cell-code}\nprefsABC$Subject <- factor(prefsABC$Subject)\nprefsABC$Pref <- factor(prefsABC$Pref)\nsummary(prefsABC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject   Pref  \n 1      : 1   A: 8  \n 2      : 1   B:21  \n 3      : 1   C:31  \n 4      : 1         \n 5      : 1         \n 6      : 1         \n (Other):54         \n```\n:::\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nggplot(prefsABC,aes(Pref))+\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\")+\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\nYou can think of the three websites as representing three bins and the preferences as filling up those bins. Either each bin gets one third of the preferences or there is a discrepancy. The Pearson $\\chi^2$ test functions as an omnibus test to tell whether there is any discrepancy in the proportions of the three bins.\n\n::: {.cell}\n\n```{.r .cell-code}\nprfs <- xtabs( ~ Pref, data=prefsABC)\nprfs # show counts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPref\n A  B  C \n 8 21 31 \n```\n:::\n\n```{.r .cell-code}\nchisq.test(prfs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tChi-squared test for given probabilities\n\ndata:  prfs\nX-squared = 13.3, df = 2, p-value = 0.001294\n```\n:::\n:::\n\nA multinomial test can test for other than an even distribution across bins. Here's an example with a one third distribution in each bin.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(XNomial)\nxmulti(prfs, c(1/3, 1/3, 1/3), statName=\"Prob\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nP value (Prob) = 0.0008024\n```\n:::\n:::\n\nNow we don't know which pair(s) differed so it makes sense to conduct post hoc binomial tests with correction for multiple comparisons. The correction, made by `p.adjust()`, is because the more hypotheses we check, the higher the probability of a Type I error, a false positive. That is, the more hypotheses we test, the higher the probability that one will appear true by chance. Wikipedia has more detail in its \"Multiple Comparisons Problem\" article.\n\nHere, we test separately for whether each one has a third of the preferences.\n\n::: {.cell}\n\n```{.r .cell-code}\naa <- binom.test(sum(prefsABC$Pref == \"A\"),\n\t\tnrow(prefsABC), p=1/3)\nbb <- binom.test(sum(prefsABC$Pref == \"B\"),\n\t\tnrow(prefsABC), p=1/3)\ncc <- binom.test(sum(prefsABC$Pref == \"C\"),\n\t\tnrow(prefsABC), p=1/3)\np.adjust(c(aa$p.value, bb$p.value, cc$p.value), method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.001659954 0.785201685 0.007446980\n```\n:::\n:::\n\nThe adjusted $p$-values tell us that A and C differ significantly from a third of the preferences.\n\n### How many males vs females prefer website A over B? (Two-sample tests of proportions in two categories)\n\nRevisit our data file with 2 response categories, but now with sex (M/F).\n\n::: {.cell}\n\n```{.r .cell-code}\nprefsABsex <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABsex.csv\"))\ntail(prefsABsex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    <dbl> <chr> <chr>\n1      55 A     M    \n2      56 B     F    \n3      57 A     M    \n4      58 B     M    \n5      59 B     M    \n6      60 A     M    \n```\n:::\n\n```{.r .cell-code}\nprefsABsex$Subject <- factor(prefsABsex$Subject)\nprefsABsex$Pref <- factor(prefsABsex$Pref)\nprefsABsex$Sex <- factor(prefsABsex$Sex)\nsummary(prefsABsex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject   Pref   Sex   \n 1      : 1   A:14   F:31  \n 2      : 1   B:46   M:29  \n 3      : 1                \n 4      : 1                \n 5      : 1                \n 6      : 1                \n (Other):54                \n```\n:::\n:::\n\nPlotting is slightly more complicated by the fact that we want to represent two groups. There are many ways to do this, including stacked bar charts, side-by-side bars, or the method chosen here, using `facet_wrap(~Sex)` to cause two separate plots based on Sex to be created.\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(prefsABsex,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  facet_wrap(~Sex) +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nAlthough we can guess by looking at the above plot that the difference for females is significant and the difference for males is not, a Pearson chi-square test provides some statistical evidence for this hunch.\n\n::: {.cell}\n\n```{.r .cell-code}\nprfs <- xtabs( ~ Pref + Sex, data=prefsABsex) # the '+' sign indicates two vars\nprfs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Sex\nPref  F  M\n   A  2 12\n   B 29 17\n```\n:::\n\n```{.r .cell-code}\nchisq.test(prfs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  prfs\nX-squared = 8.3588, df = 1, p-value = 0.003838\n```\n:::\n:::\n\n### What if the data are lopsided? (G-test, alternative to chi-square)\n\nWikipedia tells us that the $G$-test dominates the $\\chi^2$ test when $O_i>2E_i$ in the formula\n\n$$\\chi^2=\\sum_i \\frac{(O_i-E_i)^2}{E_i}$$\n\nwhere $O_i$ is the observed and $E_i$ is the expected proportion in the $i$th bin.\nThis situation may occur in small sample sizes. For large sample sizes, both tests give the same conclusion. In our case, we're on the borderline for this rule in the bin where 29 females prefer B. All females would have to prefer B for the rule to dictate a switch to the $G$-test.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RVAideMemoire)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n*** Package RVAideMemoire v 0.9-83 ***\n```\n:::\n\n```{.r .cell-code}\nG.test(prfs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tG-test\n\ndata:  prfs\nG = 11.025, df = 1, p-value = 0.0008989\n```\n:::\n\n```{.r .cell-code}\n#. Fisher's exact test\nfisher.test(prfs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  prfs\np-value = 0.001877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.009898352 0.537050159\nsample estimates:\nodds ratio \n 0.1015763 \n```\n:::\n:::\n\n### How many males vs females prefer website A, B, or C? (Two-sample tests of proportions in three categories)\n\nRevisit our data file with 3 response categories, but now with sex (M/F).\n\n::: {.cell}\n\n```{.r .cell-code}\nprefsABCsex <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABCsex.csv\"))\nhead(prefsABCsex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    <dbl> <chr> <chr>\n1       1 C     F    \n2       2 C     M    \n3       3 B     M    \n4       4 C     M    \n5       5 C     M    \n6       6 B     F    \n```\n:::\n\n```{.r .cell-code}\nprefsABCsex$Subject <- factor(prefsABCsex$Subject)\nprefsABCsex$Pref <- factor(prefsABCsex$Pref)\nprefsABCsex$Sex <- factor(prefsABCsex$Sex)\nsummary(prefsABCsex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject   Pref   Sex   \n 1      : 1   A: 8   F:29  \n 2      : 1   B:21   M:31  \n 3      : 1   C:31         \n 4      : 1                \n 5      : 1                \n 6      : 1                \n (Other):54                \n```\n:::\n\n```{.r .cell-code}\nggplot(prefsABCsex,aes(Pref)) +\n  geom_bar(width=0.5,alpha=0.4,fill=\"lightskyblue1\") +\n  facet_wrap(~Sex) +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#. Pearson chi-square test\nprfs <- xtabs( ~ Pref + Sex, data=prefsABCsex)\nprfs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Sex\nPref  F  M\n   A  3  5\n   B 15  6\n   C 11 20\n```\n:::\n\n```{.r .cell-code}\nchisq.test(prfs)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in chisq.test(prfs): Chi-squared approximation may be incorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  prfs\nX-squared = 6.9111, df = 2, p-value = 0.03157\n```\n:::\n\n```{.r .cell-code}\n#. G-test\nG.test(prfs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tG-test\n\ndata:  prfs\nG = 7.0744, df = 2, p-value = 0.02909\n```\n:::\n\n```{.r .cell-code}\n#. Fisher's exact test\nfisher.test(prfs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  prfs\np-value = 0.03261\nalternative hypothesis: two.sided\n```\n:::\n:::\n\nNow conduct manual post hoc binomial tests for (m)ales---do any prefs for A--C significantly differ from chance for males?\n\n::: {.cell}\n\n```{.r .cell-code}\nma <- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"A\"),\n\t\tnrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmb <- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"B\"),\n\t\tnrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmc <- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"C\"),\n\t\tnrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(ma$p.value, mb$p.value, mc$p.value), method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.109473564 0.126622172 0.001296754\n```\n:::\n:::\n\nNext, conduct manual post hoc binomial tests for (f)emales---do any prefs for A--C significantly differ from chance for females?\n\n::: {.cell}\n\n```{.r .cell-code}\nfa <- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"A\"),\n\t\tnrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfb <- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"B\"),\n\t\tnrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfc <- binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"C\"),\n\t\tnrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(fa$p.value, fb$p.value, fc$p.value), method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02703274 0.09447821 0.69396951\n```\n:::\n:::\n\n\n\n## How do groups compare in reading performance? (Independent samples $t$-test)\nHere we are asking which group read more pages on a particular website.\n\n::: {.cell}\n\n```{.r .cell-code}\npgviews <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/pgviews.csv\"))\npgviews$Subject <- factor(pgviews$Subject)\npgviews$Site <- factor(pgviews$Site)\nsummary(pgviews)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject    Site        Pages       \n 1      :  1   A:245   Min.   : 1.000  \n 2      :  1   B:255   1st Qu.: 3.000  \n 3      :  1           Median : 4.000  \n 4      :  1           Mean   : 3.958  \n 5      :  1           3rd Qu.: 5.000  \n 6      :  1           Max.   :11.000  \n (Other):494                           \n```\n:::\n\n```{.r .cell-code}\ntail(pgviews)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  Subject Site  Pages\n  <fct>   <fct> <dbl>\n1 495     A         3\n2 496     B         6\n3 497     B         6\n4 498     A         3\n5 499     A         4\n6 500     B         6\n```\n:::\n\n```{.r .cell-code}\n#. descriptive statistics by Site\nplyr::ddply(pgviews, ~ Site, function(data) summary(data$Pages))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Site Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1    A    1       3      3 3.404082       4    6\n2    B    1       3      4 4.490196       6   11\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(pgviews, ~ Site, summarise, Pages.mean=mean(Pages), Pages.sd=sd(Pages))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Site Pages.mean Pages.sd\n1    A   3.404082 1.038197\n2    B   4.490196 2.127552\n```\n:::\n\n```{.r .cell-code}\n#. graph histograms and a boxplot\nggplot(pgviews,aes(Pages,fill=Site,color=Site)) +\n  geom_bar(alpha=0.5,position=\"identity\",color=\"white\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(pgviews,aes(Site,Pages,fill=Site)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-16-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#. independent-samples t-test\nt.test(Pages ~ Site, data=pgviews, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  Pages by Site\nt = -7.2083, df = 498, p-value = 2.115e-12\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.3821544 -0.7900745\nsample estimates:\nmean in group A mean in group B \n       3.404082        4.490196 \n```\n:::\n:::\n\n\n\n## ANOVA\nANOVA stands for analysis of variance and is a way to generalize the $t$-test to more groups.\n\n### How long does it take to perform tasks on two IDEs?\n\n::: {.cell}\n\n```{.r .cell-code}\nide2 <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/ide2.csv\"))\nide2$Subject <- factor(ide2$Subject) # convert to nominal factor\nide2$IDE <- factor(ide2$IDE) # convert to nominal factor\nsummary(ide2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject        IDE          Time      \n 1      : 1   Eclipse:20   Min.   :155.0  \n 2      : 1   VStudio:20   1st Qu.:271.8  \n 3      : 1                Median :313.5  \n 4      : 1                Mean   :385.1  \n 5      : 1                3rd Qu.:422.0  \n 6      : 1                Max.   :952.0  \n (Other):34                               \n```\n:::\n\n```{.r .cell-code}\n#. view descriptive statistics by IDE\nplyr::ddply(ide2, ~ IDE, function(data) summary(data$Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      IDE Min. 1st Qu. Median   Mean 3rd Qu. Max.\n1 Eclipse  232  294.75  393.5 468.15  585.50  952\n2 VStudio  155  246.50  287.0 302.10  335.25  632\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(ide2, ~ IDE, summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      IDE Time.mean  Time.sd\n1 Eclipse    468.15 218.1241\n2 VStudio    302.10 101.0778\n```\n:::\n\n```{.r .cell-code}\n#. graph histograms and a boxplot\nggplot(ide2,aes(Time,fill=IDE)) +\n  geom_histogram(binwidth=50,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Paired\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(ide2,aes(IDE,Time,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#. independent-samples t-test (suitable? maybe not, because...)\nt.test(Time ~ IDE, data=ide2, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  Time by IDE\nt = 3.0889, df = 38, p-value = 0.003745\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n  57.226 274.874\nsample estimates:\nmean in group Eclipse mean in group VStudio \n               468.15                302.10 \n```\n:::\n:::\n\n### Testing ANOVA assumptions\n\n::: {.cell}\n\n```{.r .cell-code}\n#. Shapiro-Wilk normality test on response\nshapiro.test(ide2[ide2$IDE == \"VStudio\",]$Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$Time\nW = 0.84372, p-value = 0.004191\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(ide2[ide2$IDE == \"Eclipse\",]$Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$Time\nW = 0.87213, p-value = 0.01281\n```\n:::\n\n```{.r .cell-code}\n#. but really what matters most is the residuals\nm = aov(Time ~ IDE, data=ide2) # fit model\nshapiro.test(residuals(m)) # test residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.894, p-value = 0.001285\n```\n:::\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n### Kolmogorov-Smirnov test for log-normality\nFit the distribution to a lognormal to estimate fit parameters\nthen supply those to a K-S test with the lognormal distribution fn (see ?plnorm).\nSee ?distributions for many other named probability distributions.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'MASS'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n:::\n\n```{.r .cell-code}\nfit <- fitdistr(ide2[ide2$IDE == \"VStudio\",]$Time,\n\t       \"lognormal\")$estimate\nks.test(ide2[ide2$IDE == \"VStudio\",]$Time, \"plnorm\",\n\tmeanlog=fit[1], sdlog=fit[2], exact=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$Time\nD = 0.13421, p-value = 0.8181\nalternative hypothesis: two-sided\n```\n:::\n\n```{.r .cell-code}\nfit <- fitdistr(ide2[ide2$IDE == \"Eclipse\",]$Time,\n\t       \"lognormal\")$estimate\nks.test(ide2[ide2$IDE == \"Eclipse\",]$Time, \"plnorm\",\n\tmeanlog=fit[1], sdlog=fit[2], exact=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$Time\nD = 0.12583, p-value = 0.871\nalternative hypothesis: two-sided\n```\n:::\n\n```{.r .cell-code}\n#. tests for homoscedasticity (homogeneity of variance)\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'car'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    recode\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    some\n```\n:::\n\n```{.r .cell-code}\nleveneTest(Time ~ IDE, data=ide2, center=mean) # Levene's test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value   Pr(>F)   \ngroup  1  11.959 0.001356 **\n      38                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nleveneTest(Time ~ IDE, data=ide2, center=median) # Brown-Forsythe test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(>F)  \ngroup  1  5.9144 0.01984 *\n      38                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n#. Welch t-test for unequal variances handles\n#. the violation of homoscedasticity. but not\n#. the violation of normality.\nt.test(Time ~ IDE, data=ide2, var.equal=FALSE) # Welch t-test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  Time by IDE\nt = 3.0889, df = 26.8, p-value = 0.004639\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n  55.71265 276.38735\nsample estimates:\nmean in group Eclipse mean in group VStudio \n               468.15                302.10 \n```\n:::\n:::\n\n### Data transformation\n\n::: {.cell}\n\n```{.r .cell-code}\n#. create a new column in ide2 defined as log(Time)\nide2$logTime <- log(ide2$Time) # log transform\nhead(ide2) # verify\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  Subject IDE      Time logTime\n  <fct>   <fct>   <dbl>   <dbl>\n1 1       VStudio   341    5.83\n2 2       VStudio   291    5.67\n3 3       VStudio   283    5.65\n4 4       VStudio   155    5.04\n5 5       VStudio   271    5.60\n6 6       VStudio   270    5.60\n```\n:::\n\n```{.r .cell-code}\n#. explore for intuition-building\nggplot(ide2,aes(logTime,fill=IDE)) +\n  geom_histogram(binwidth=0.2,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Paired\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(ide2,aes(IDE,logTime,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#. re-test for normality\nshapiro.test(ide2[ide2$IDE == \"VStudio\",]$logTime)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"VStudio\", ]$logTime\nW = 0.95825, p-value = 0.5094\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(ide2[ide2$IDE == \"Eclipse\",]$logTime)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  ide2[ide2$IDE == \"Eclipse\", ]$logTime\nW = 0.93905, p-value = 0.23\n```\n:::\n\n```{.r .cell-code}\nm <- aov(logTime ~ IDE, data=ide2) # fit model\nshapiro.test(residuals(m)) # test residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.96218, p-value = 0.1987\n```\n:::\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-20-3.png){width=672}\n:::\n\n```{.r .cell-code}\n#. re-test for homoscedasticity\nleveneTest(logTime ~ IDE, data=ide2, center=median) # Brown-Forsythe test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(>F)  \ngroup  1  3.2638 0.07875 .\n      38                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n#. independent-samples t-test (now suitable for logTime)\nt.test(logTime ~ IDE, data=ide2, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  logTime by IDE\nt = 3.3121, df = 38, p-value = 0.002039\nalternative hypothesis: true difference in means between group Eclipse and group VStudio is not equal to 0\n95 percent confidence interval:\n 0.1514416 0.6276133\nsample estimates:\nmean in group Eclipse mean in group VStudio \n             6.055645              5.666118 \n```\n:::\n:::\n\n### What if ANOVA assumptions don't hold? (Nonparametric equivalent of independent-samples t-test)\n\n### Mann-Whitney U test\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coin)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: survival\n```\n:::\n\n```{.r .cell-code}\nwilcox_test(Time ~ IDE, data=ide2, distribution=\"exact\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact Wilcoxon-Mann-Whitney Test\n\ndata:  Time by IDE (Eclipse, VStudio)\nZ = 2.9487, p-value = 0.002577\nalternative hypothesis: true mu is not equal to 0\n```\n:::\n\n```{.r .cell-code}\nwilcox_test(logTime ~ IDE, data=ide2, distribution=\"exact\") # note: same result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact Wilcoxon-Mann-Whitney Test\n\ndata:  logTime by IDE (Eclipse, VStudio)\nZ = 2.9487, p-value = 0.002577\nalternative hypothesis: true mu is not equal to 0\n```\n:::\n:::\n\n### How long does it take to do tasks on one of three tools? (One-way ANOVA preparation)\n\n::: {.cell}\n\n```{.r .cell-code}\n#. read in a data file with task completion times (min) now from 3 tools\nide3 <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/ide3.csv\"))\nide3$Subject <- factor(ide3$Subject) # convert to nominal factor\nide3$IDE <- factor(ide3$IDE) # convert to nominal factor\nsummary(ide3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject        IDE          Time      \n 1      : 1   Eclipse:20   Min.   :143.0  \n 2      : 1   PyCharm:20   1st Qu.:248.8  \n 3      : 1   VStudio:20   Median :295.0  \n 4      : 1                Mean   :353.9  \n 5      : 1                3rd Qu.:391.2  \n 6      : 1                Max.   :952.0  \n (Other):54                               \n```\n:::\n\n```{.r .cell-code}\n#. view descriptive statistics by IDE\nplyr::ddply(ide3, ~ IDE, function(data) summary(data$Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      IDE Min. 1st Qu. Median   Mean 3rd Qu. Max.\n1 Eclipse  232  294.75  393.5 468.15  585.50  952\n2 PyCharm  143  232.25  279.5 291.45  300.00  572\n3 VStudio  155  246.50  287.0 302.10  335.25  632\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(ide3, ~ IDE, summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      IDE Time.mean  Time.sd\n1 Eclipse    468.15 218.1241\n2 PyCharm    291.45 106.8922\n3 VStudio    302.10 101.0778\n```\n:::\n\n```{.r .cell-code}\nide3 |>\n  group_by(IDE) |>\n  summarize(median=median(Time),mean=mean(Time),sd=sd(Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  IDE     median  mean    sd\n  <fct>    <dbl> <dbl> <dbl>\n1 Eclipse   394.  468.  218.\n2 PyCharm   280.  291.  107.\n3 VStudio   287   302.  101.\n```\n:::\n\n```{.r .cell-code}\n#. explore new response distribution\nggplot(ide3,aes(Time,fill=IDE)) +\n  geom_histogram(binwidth=50,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(ide3,aes(IDE,Time,fill=IDE)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-22-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#. test normality for new IDE\nshapiro.test(ide3[ide3$IDE == \"PyCharm\",]$Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$Time\nW = 0.88623, p-value = 0.02294\n```\n:::\n\n```{.r .cell-code}\nm <- aov(Time ~ IDE, data=ide3) # fit model\nshapiro.test(residuals(m)) # test residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.89706, p-value = 0.000103\n```\n:::\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-22-3.png){width=672}\n:::\n\n```{.r .cell-code}\n#. test log-normality of new IDE\nfit <- fitdistr(ide3[ide3$IDE == \"PyCharm\",]$Time, \"lognormal\")$estimate\nks.test(ide3[ide3$IDE == \"PyCharm\",]$Time,\n\t\"plnorm\", meanlog=fit[1], sdlog=fit[2], exact=TRUE) # lognormality\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$Time\nD = 0.1864, p-value = 0.4377\nalternative hypothesis: two-sided\n```\n:::\n\n```{.r .cell-code}\n#. compute new log(Time) column and re-test\nide3$logTime <- log(ide3$Time) # add new column\nshapiro.test(ide3[ide3$IDE == \"PyCharm\",]$logTime)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  ide3[ide3$IDE == \"PyCharm\", ]$logTime\nW = 0.96579, p-value = 0.6648\n```\n:::\n\n```{.r .cell-code}\nm <- aov(logTime ~ IDE, data=ide3) # fit model\nshapiro.test(residuals(m)) # test residuals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.96563, p-value = 0.08893\n```\n:::\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nqqnorm(residuals(m)); qqline(residuals(m)) # plot residuals\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-22-4.png){width=672}\n:::\n\n```{.r .cell-code}\n#. test homoscedasticity\nleveneTest(logTime ~ IDE, data=ide3, center=median) # Brown-Forsythe test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  2  1.7797 0.1779\n      57               \n```\n:::\n:::\n\n### Can we transform data so it fits assumptions? (One-way ANOVA, suitable now to logTime)\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- aov(logTime ~ IDE, data=ide3) # fit model\nanova(m) # report anova\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: logTime\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nIDE        2 2.3064  1.1532   8.796 0.0004685 ***\nResiduals 57 7.4729  0.1311                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n#. post hoc independent-samples t-tests\nlibrary(multcomp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mvtnorm\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: TH.data\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'TH.data'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:MASS':\n\n    geyser\n```\n:::\n\n```{.r .cell-code}\nsummary(glht(m, mcp(IDE=\"Tukey\")), test=adjusted(type=\"holm\")) # Tukey means compare all pairs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = logTime ~ IDE, data = ide3)\n\nLinear Hypotheses:\n                       Estimate Std. Error t value Pr(>|t|)    \nPyCharm - Eclipse == 0  -0.4380     0.1145  -3.826 0.000978 ***\nVStudio - Eclipse == 0  -0.3895     0.1145  -3.402 0.002458 ** \nVStudio - PyCharm == 0   0.0485     0.1145   0.424 0.673438    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n```\n:::\n\n```{.r .cell-code}\n#. note: equivalent to this using lsm instead of mcp\nlibrary(emmeans)\nsummary(glht(m, lsm(pairwise ~ IDE)), test=adjusted(type=\"holm\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nFit: aov(formula = logTime ~ IDE, data = ide3)\n\nLinear Hypotheses:\n                       Estimate Std. Error t value Pr(>|t|)    \nEclipse - PyCharm == 0   0.4380     0.1145   3.826 0.000978 ***\nEclipse - VStudio == 0   0.3895     0.1145   3.402 0.002458 ** \nPyCharm - VStudio == 0  -0.0485     0.1145  -0.424 0.673438    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n```\n:::\n:::\n\n### What if we can't transform data to fit ANOVA assumptions? (Nonparametric equivalent of one-way ANOVA)\n\n::: {.cell}\n\n```{.r .cell-code}\n#. Kruskal-Wallis test\nkruskal_test(Time ~ IDE, data=ide3, distribution=\"asymptotic\") # can't do exact with 3 levels\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAsymptotic Kruskal-Wallis Test\n\ndata:  Time by IDE (Eclipse, PyCharm, VStudio)\nchi-squared = 12.17, df = 2, p-value = 0.002277\n```\n:::\n\n```{.r .cell-code}\nkruskal_test(logTime ~ IDE, data=ide3, distribution=\"asymptotic\") # note: same result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAsymptotic Kruskal-Wallis Test\n\ndata:  logTime by IDE (Eclipse, PyCharm, VStudio)\nchi-squared = 12.17, df = 2, p-value = 0.002277\n```\n:::\n\n```{.r .cell-code}\n#. for reporting Kruskal-Wallis as chi-square, we can get N with nrow(ide3)\n\n#. manual post hoc Mann-Whitney U pairwise comparisons\n#. note: wilcox_test we used above doesn't take two data vectors, so use wilcox.test\nvs.ec <- wilcox.test(ide3[ide3$IDE == \"VStudio\",]$Time,\n\t\t    ide3[ide3$IDE == \"Eclipse\",]$Time, exact=FALSE)\nvs.py <- wilcox.test(ide3[ide3$IDE == \"VStudio\",]$Time,\n\t\t    ide3[ide3$IDE == \"PyCharm\",]$Time, exact=FALSE)\nec.py <- wilcox.test(ide3[ide3$IDE == \"Eclipse\",]$Time,\n\t\t    ide3[ide3$IDE == \"PyCharm\",]$Time, exact=FALSE)\np.adjust(c(vs.ec$p.value, vs.py$p.value, ec.py$p.value), method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.007681846 0.588488864 0.007681846\n```\n:::\n\n```{.r .cell-code}\n#. alternative approach is using PMCMRplus for nonparam pairwise comparisons\nlibrary(PMCMRplus)\nkwAllPairsConoverTest(Time ~ IDE, data=ide3, p.adjust.method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in kwAllPairsConoverTest.default(c(341, 291, 283, 155, 271, 270, : Ties\nare present. Quantiles were corrected for ties.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n\tPairwise comparisons using Conover's all-pairs test\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndata: Time by IDE\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n        Eclipse PyCharm\nPyCharm 0.0025  -      \nVStudio 0.0062  0.6620 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nP value adjustment method: holm\n```\n:::\n:::\n\nThe above test was reported by W. J. Conover and R. L. Iman (1979), _On multiple-comparisons\n     procedures_, Tech. Rep. LA-7677-MS, Los Alamos Scientific Laboratory. \n\n\n\n### Another example of tasks using two tools (More on oneway ANOVA)\n\nThe `designtime` data records task times in minutes to complete the same project in Illustrator or InDesign.\n\nRead the designtime data into R. Determine how many subjects participated.\n\n::: {.cell}\n\n```{.r .cell-code}\ndt <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/designtime.csv\"))\n#. convert Subject to a factor\ndt$Subject<-as.factor(dt$Subject)\ndt$Tool<-as.factor(dt$Tool)\nsummary(dt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject            Tool         Time       \n 1      : 1   Illustrator:30   Min.   : 98.19  \n 2      : 1   InDesign   :30   1st Qu.:149.34  \n 3      : 1                    Median :205.54  \n 4      : 1                    Mean   :275.41  \n 5      : 1                    3rd Qu.:361.99  \n 6      : 1                    Max.   :926.15  \n (Other):54                                    \n```\n:::\n\n```{.r .cell-code}\nlength(dt$Subject)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 60\n```\n:::\n\n```{.r .cell-code}\ntail(dt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  Subject Tool         Time\n  <fct>   <fct>       <dbl>\n1 55      Illustrator  218.\n2 56      InDesign     180.\n3 57      Illustrator  170.\n4 58      InDesign     186.\n5 59      Illustrator  241.\n6 60      InDesign     159.\n```\n:::\n:::\n\nWe see from the summary that there are sixty observations. We can see the same by checking the `length()` of the Subject (or any other) variable in the data.\n\nCreate a boxplot of the task time for each tool and comment on the medians and variances.\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dt,aes(Tool,Time,fill=Tool)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\nBoth the median and the variance is much larger for Illustrator than for InDesign.\n\nConduct a Shapiro-Wilk test for normality for each tool and comment.\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(dt[dt$Tool==\"Illustrator\",]$Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  dt[dt$Tool == \"Illustrator\", ]$Time\nW = 0.90521, p-value = 0.01129\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(dt[dt$Tool==\"InDesign\",]$Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  dt[dt$Tool == \"InDesign\", ]$Time\nW = 0.95675, p-value = 0.2553\n```\n:::\n:::\n\nIn the case of InDesign, we fail to reject the null hypothesis that the data are drawn from a normal distribution. In the case of Illustrator, we reject the null hypothesis at the five percent level but not at the one percent level (just barely).\n\nConduct a Shapiro-Wilk test for normality on the residuals and comment.\n\n::: {.cell}\n\n```{.r .cell-code}\nm<-aov(Time~Tool,data=dt)\nshapiro.test(residuals(m))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.85077, p-value = 3.211e-06\n```\n:::\n:::\n\nWe reject the null hypothesis that the residuals are normally distributed.\n\nConduct a Brown-Forsythe test of homoscedasticity.\n\n::: {.cell}\n\n```{.r .cell-code}\nleveneTest(Time~Tool,data=dt,center=median)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value    Pr(>F)    \ngroup  1  20.082 3.545e-05 ***\n      58                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nWe reject the null hypothesis that the two samples are drawn from populations with equal variance.\n\nFit a lognormal distribution to the Time response for each Tool. Conduct a Kolmogorov-Smirnov goodness-of-fit test and comment.\n\n::: {.cell}\n\n```{.r .cell-code}\nfit<-fitdistr(dt[dt$Tool==\"Illustrator\",]$Time,\n    \"lognormal\")$estimate\ntst<-ks.test(dt[dt$Tool==\"Illustrator\",]$Time,\n    \"plnorm\",meanlog=fit[1],sdlog=fit[2],exact=TRUE)\ntst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  dt[dt$Tool == \"Illustrator\", ]$Time\nD = 0.093358, p-value = 0.9344\nalternative hypothesis: two-sided\n```\n:::\n\n```{.r .cell-code}\nfit<-fitdistr(dt[dt$Tool==\"InDesign\",]$Time,\n    \"lognormal\")$estimate\ntst<-ks.test(dt[dt$Tool==\"InDesign\",]$Time,\n    \"plnorm\",meanlog=fit[1],sdlog=fit[2],exact=TRUE)\ntst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  dt[dt$Tool == \"InDesign\", ]$Time\nD = 0.10005, p-value = 0.8958\nalternative hypothesis: two-sided\n```\n:::\n:::\n\nWe fail to reject the null hypothesis that the Illustrator sample is drawn from a lognormal distribution.\nWe fail to reject the null hypothesis that the InDesign sample is drawn from a lognormal distribution.\n\nCreate a log-transformed Time response column. Compute the mean for each tool and comment.\n\n::: {.cell}\n\n```{.r .cell-code}\ndt$logTime<-log(dt$Time)\nmean(dt$logTime[dt$Tool==\"Illustrator\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.894288\n```\n:::\n\n```{.r .cell-code}\nmean(dt$logTime[dt$Tool==\"InDesign\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.03047\n```\n:::\n\n```{.r .cell-code}\ndt |>\n  group_by(Tool) |>\n  summarize(mean=mean(logTime),sd=sd(logTime))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  Tool         mean    sd\n  <fct>       <dbl> <dbl>\n1 Illustrator  5.89 0.411\n2 InDesign     5.03 0.211\n```\n:::\n:::\n\nThe mean for Illustrator appears to be larger than the mean for InDesign.\n\nConduct an independent-samples $t$-test on the log-transformed Time response, using the Welch version for unequal variances and comment.\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(logTime~Tool,data=dt,var.equal=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  logTime by Tool\nt = 10.23, df = 43.293, p-value = 3.98e-13\nalternative hypothesis: true difference in means between group Illustrator and group InDesign is not equal to 0\n95 percent confidence interval:\n 0.6935646 1.0340718\nsample estimates:\nmean in group Illustrator    mean in group InDesign \n                 5.894288                  5.030470 \n```\n:::\n:::\n\nWe reject the null hypothesis that the true difference in means is equal to 0.\n\nConduct an exact nonparametric Mann-Whitney $U$ test on the Time response and comment.\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox_test(Time~Tool,data=dt,distribution=\"exact\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact Wilcoxon-Mann-Whitney Test\n\ndata:  Time by Tool (Illustrator, InDesign)\nZ = 6.3425, p-value = 5.929e-14\nalternative hypothesis: true mu is not equal to 0\n```\n:::\n:::\n\nWe reject the null hypothesis that the samples were drawn from populations with the same distribution.\n\n\n\n### Differences in writing speed among three tools (Three levels of a factor in ANOVA)\n\nWe'll examine three levels of a factor, which is an alphabet system used for writing. The three levels are named for the text entry systems, EdgeWrite, Graffiti, and Unistrokes.\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/alphabets.csv\"))\nalpha$Subject<-as.factor(alpha$Subject)\nalpha$Alphabet<-as.factor(alpha$Alphabet)\nsummary(alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject         Alphabet       WPM        \n 1      : 1   EdgeWrite :20   Min.   : 3.960  \n 2      : 1   Graffiti  :20   1st Qu.: 9.738  \n 3      : 1   Unistrokes:20   Median :13.795  \n 4      : 1                   Mean   :14.517  \n 5      : 1                   3rd Qu.:18.348  \n 6      : 1                   Max.   :28.350  \n (Other):54                                   \n```\n:::\n:::\n\nPlot the three text entry systems.\n::: {.cell}\n\n```{.r .cell-code}\nggplot(alpha,aes(Alphabet,WPM,fill=Alphabet)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\nIdentify the average words per minute written with EdgeWrite.\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(alpha[alpha$Alphabet==\"EdgeWrite\",]$WPM)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17.14\n```\n:::\n:::\n\nConduct a Shapiro-Wilk test for normality on each method.\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"EdgeWrite\"]\nW = 0.95958, p-value = 0.5355\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(alpha$WPM[alpha$Alphabet==\"Graffiti\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"Graffiti\"]\nW = 0.94311, p-value = 0.2743\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(alpha$WPM[alpha$Alphabet==\"Unistrokes\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  alpha$WPM[alpha$Alphabet == \"Unistrokes\"]\nW = 0.94042, p-value = 0.2442\n```\n:::\n:::\n\nConduct a Shapiro-Wilk test for normality on the residuals of an ANOVA model stipulating that Alphabet affects WPM.\n\n::: {.cell}\n\n```{.r .cell-code}\nm<-aov(WPM~Alphabet,data=alpha)\nshapiro.test(residuals(m))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.97762, p-value = 0.3363\n```\n:::\n:::\n\nTest for homoscedasticity.\n\n::: {.cell}\n\n```{.r .cell-code}\nleveneTest(alpha$WPM~alpha$Alphabet,center=\"median\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = \"median\")\n      Df F value Pr(>F)\ngroup  2  1.6219 0.2065\n      57               \n```\n:::\n:::\n\nNow test all three. The `mcp` function tests multiple means. The keyword `Tukey` means to do all the possible pairwise comparisons of Alphabet, i.e., Graffiti and EdgeWrite, Graffiti and Unistrokes, and EdgeWrite and Unistrokes. `m` is the oneway ANOVA model we created above.\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(multcomp::glht(m,multcomp::mcp(Alphabet=\"Tukey\")),test=adjusted(type=\"holm\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = WPM ~ Alphabet, data = alpha)\n\nLinear Hypotheses:\n                            Estimate Std. Error t value Pr(>|t|)   \nGraffiti - EdgeWrite == 0     -2.101      1.693  -1.241  0.21982   \nUnistrokes - EdgeWrite == 0   -5.769      1.693  -3.407  0.00363 **\nUnistrokes - Graffiti == 0    -3.668      1.693  -2.166  0.06894 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n```\n:::\n:::\n\nConduct a nonparametric oneway ANOVA using the Kruskal-Wallis test to see if the samples have the same distribution. The null hypothesis is that the samples come from the same distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal_test(alpha$WPM~alpha$Alphabet,distribution=\"asymptotic\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAsymptotic Kruskal-Wallis Test\n\ndata:  alpha$WPM by\n\t alpha$Alphabet (EdgeWrite, Graffiti, Unistrokes)\nchi-squared = 9.7019, df = 2, p-value = 0.007821\n```\n:::\n:::\n\nConduct manual post hoc Mann-Whitney pairwise comparisons and adjust the $p$-values to take into account the possibility of false discovery.\n\n::: {.cell}\n\n```{.r .cell-code}\newgf<-wilcox.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"],alpha$WPM[alpha$Alphabet==\"Graffiti\"],paired=FALSE,exact=FALSE)\newun<-wilcox.test(alpha$WPM[alpha$Alphabet==\"EdgeWrite\"],alpha$WPM[alpha$Alphabet==\"Unistrokes\"],paired=FALSE,exact=FALSE)\ngfun<-wilcox.test(alpha$WPM[alpha$Alphabet==\"Graffiti\"],alpha$WPM[alpha$Alphabet==\"Unistrokes\"],paired=FALSE,exact=FALSE)\np.adjust(c(ewgf$p.value,ewun$p.value,gfun$p.value),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.20358147 0.01810677 0.04146919\n```\n:::\n:::\n\n\n\n## Same person using two different tools (Paired samples $t$-test)\n\nIs it better to search or scroll for contacts in a smartphone contacts manager?\nWhich takes more time? Which takes more effort? Which is more error-prone?\nStart by reading in data, converting to factors, and summarizing.\n\n::: {.cell}\n\n```{.r .cell-code}\nsrchscrl <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/srchscrl.csv\"))\nsrchscrl$Subject <- factor(srchscrl$Subject)\nsrchscrl$Order   <- factor(srchscrl$Order)\nsrchscrl$Technique   <- factor(srchscrl$Technique)\n#. srchscrl$Errors   <- factor(srchscrl$Errors,ordered=TRUE,levels=c(0,1,2,3,4))\nsummary(srchscrl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject    Technique  Order       Time           Errors         Effort \n 1      : 2   Scroll:20   1:20   Min.   : 49.0   Min.   :0.00   Min.   :1  \n 2      : 2   Search:20   2:20   1st Qu.: 94.5   1st Qu.:0.75   1st Qu.:3  \n 3      : 2                      Median :112.5   Median :1.50   Median :4  \n 4      : 2                      Mean   :117.0   Mean   :1.60   Mean   :4  \n 5      : 2                      3rd Qu.:148.2   3rd Qu.:2.25   3rd Qu.:5  \n 6      : 2                      Max.   :192.0   Max.   :4.00   Max.   :7  \n (Other):28                                                                \n```\n:::\n:::\n\n\n```{.r .cell-code}\nlibrary(xtable)\noptions(xtable.comment=FALSE)\noptions(xtable.booktabs=TRUE)\nxtable(head(srchscrl),caption=\"First rows of data\")\n```\n\n\\begin{table}[ht]\n\\centering\n\\begin{tabular}{rlllrrr}\n  \\toprule\n & Subject & Technique & Order & Time & Errors & Effort \\\\ \n  \\midrule\n1 & 1 & Search & 1 & 98.00 & 4.00 & 5.00 \\\\ \n  2 & 1 & Scroll & 2 & 152.00 & 0.00 & 6.00 \\\\ \n  3 & 2 & Search & 2 & 57.00 & 2.00 & 2.00 \\\\ \n  4 & 2 & Scroll & 1 & 148.00 & 0.00 & 3.00 \\\\ \n  5 & 3 & Search & 1 & 86.00 & 3.00 & 2.00 \\\\ \n  6 & 3 & Scroll & 2 & 160.00 & 0.00 & 4.00 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{First rows of data} \n\\end{table}\n\nView descriptive statistics by Technique. There are several ways to do this. The following uses the `plyr` package.\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(srchscrl, ~ Technique,\n      function(data) summary(data$Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Min. 1st Qu. Median  Mean 3rd Qu. Max.\n1    Scroll   49  123.50  148.5 137.2     161  192\n2    Search   50   86.75   99.5  96.8     106  147\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(srchscrl, ~ Technique,\n      summarise, Time.mean=mean(Time), Time.sd=sd(Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Time.mean  Time.sd\n1    Scroll     137.2 35.80885\n2    Search      96.8 23.23020\n```\n:::\n:::\n\nAnother approach is to use the `dplyr` package. Be aware that it conflicts with `plyr` so you should try to avoid using both. If you must use both, as I did above, it may make the most sense to call particular functions from the `plyr` package rather than load the package. This is what I did with `plyr::ddply()` above.\n\n::: {.cell}\n\n```{.r .cell-code}\nsrchscrl |>\n  group_by(Technique) |>\n  summarize(mean=mean(Time),sd=sd(Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  Technique  mean    sd\n  <fct>     <dbl> <dbl>\n1 Scroll    137.   35.8\n2 Search     96.8  23.2\n```\n:::\n:::\n\nYou can explore the Time response by making histograms or boxplots.\nOne approach is to use the `ggplot2` package and put the histograms together in one frame. The `ggplot2` package allows for a remarkable variety of options.\n\n::: {.cell fig.margin='false'}\n\n```{.r .cell-code}\nggplot(srchscrl,aes(Time,fill=Technique)) +\n  geom_histogram(bins=30,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\nWe can use the same package for boxplots. Boxplots show the median as a bold line in the middle of the box. The box itself ranges from the first quartile (starting at the 25th percentile) to the third quartile (terminating at the 75th percentile). The whiskers run from the minimum to the maximum, where these are defined as the 25th percentile minus 1.5 times the interquartile range and the 75th percentile plus 1.5 times the interquartile range. The interquartile range is the width of the box. Dots outside the whiskers show outliers.\n\n::: {.cell fig.margin='false'}\n\n```{.r .cell-code}\nggplot(srchscrl,aes(Technique,Time,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\nWe would rather use parametric statistics if ANOVA assumptions are met. Recall that we can test for normality, normality of residuals, and homoscedasticity. In the case of a within-subjects experiment, we can also test for order effects which is one way to test the independence assumption. First test whether these times seem to be drawn from a normal distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(srchscrl[srchscrl$Technique == \"Search\",]$Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  srchscrl[srchscrl$Technique == \"Search\", ]$Time\nW = 0.96858, p-value = 0.7247\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(srchscrl[srchscrl$Technique == \"Scroll\",]$Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  srchscrl[srchscrl$Technique == \"Scroll\", ]$Time\nW = 0.91836, p-value = 0.09213\n```\n:::\n:::\n\nIn both cases we fail to reject the null hypothesis, which is that the Time data are drawn from a normal distribution. Note that we fail to reject at $\\alpha=0.05$ but that in the case of the Scroll technique we would reject at $\\alpha=0.1$.\n\nFit a model for testing residuals---the Error function is used\nto indicate within-subject effects, i.e., each Subject was\nexposed to all levels of Technique. generally, Error(S/(A*B*C))\nmeans each S was exposed to every level of A, B, C and S\nis a column encoding subject ids.\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- aov(Time ~ Technique + Error(Subject/Technique),\n\tdata=srchscrl)\n```\n:::\n\nThe above-specified model has residuals---departures of the observed data from the data that would be expected if the model were accurate.\n\nNow we can test the residuals of this model for normality and also examine a QQ plot for normality. The QQ plot shows the theoretical line to which the residuals should adhere if they are normally distributed. Deviations from that line are indications of non-normality. First test by Subject.\n\n::: {.cell fig.margin='true'}\n\n```{.r .cell-code}\nshapiro.test(residuals(m$Subject))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m$Subject)\nW = 0.9603, p-value = 0.5783\n```\n:::\n\n```{.r .cell-code}\nqqnorm(residuals(m$Subject)) \nqqline(residuals(m$Subject))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\nWe fail to reject the null hypothesis of normality and the QQ plot looks normal. So far, so good.\n\nNext test by Subject:Technique.\n\n::: {.cell fig.margin='true'}\n\n```{.r .cell-code}\nshapiro.test(residuals(m$'Subject:Technique'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m$\"Subject:Technique\")\nW = 0.97303, p-value = 0.8172\n```\n:::\n\n```{.r .cell-code}\nqqnorm(residuals(m$'Subject:Technique'))\nqqline(residuals(m$'Subject:Technique'))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\nWe fail to reject the null hypothesis of normality and the QQ plot looks normal. We're getting there.\n\nWe're still checking the ANOVA assumptions. Next thing to test is homoscedasticity, the assumption of equal variance. For this we use the Brown-Forsythe test, a variant of Levene's test that uses the median instead of the mean, providing greater robustness against non-normal data.\n\n::: {.cell}\n\n```{.r .cell-code}\nleveneTest(Time ~ Technique, data=srchscrl, center=median)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  2.0088 0.1645\n      38               \n```\n:::\n:::\n\nThis experiment used counterbalancing to ward off the possibility of an order effect. An order effect results from learning or fatigue or some other factor based on the order in which the tests were run. We would like to not have that happen and one solution is to have half the subjects do task A first and half the subjects do task B first. This is the simplest form of counterbalancing. It becomes more problematic if there are more than two tasks.\n\nFor a paired-samples $t$-test we must use a wide-format table; most\nR functions do not require a wide-format table, but the `dcast()` function\noffers a quick way to translate long-format into wide-format when\nwe need it.\n\nA wide-format table has one subject in every row. A long-format table has one observation in every row. Most R functions use long-format tables.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'reshape2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tidyr':\n\n    smiths\n```\n:::\n\n```{.r .cell-code}\nsrchscrl.wide.order <- dcast(srchscrl, Subject ~ Order,\n\t\t\t     value.var=\"Time\")\n```\n:::\n\n\n```{.r .cell-code}\nxtable(head(srchscrl.wide.order),\n       caption=\"First rows of wide order\")\n```\n\n\\begin{table}[ht]\n\\centering\n\\begin{tabular}{rlrr}\n  \\toprule\n & Subject & 1 & 2 \\\\ \n  \\midrule\n1 & 1 & 98.00 & 152.00 \\\\ \n  2 & 2 & 148.00 & 57.00 \\\\ \n  3 & 3 & 86.00 & 160.00 \\\\ \n  4 & 4 & 113.00 & 65.00 \\\\ \n  5 & 5 & 102.00 & 181.00 \\\\ \n  6 & 6 & 70.00 & 126.00 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{First rows of wide order} \n\\end{table}\n\nNow conduct a $t$-test to see if order has an effect.\n::: {.cell}\n\n```{.r .cell-code}\nt.test(srchscrl.wide.order$\"1\", srchscrl.wide.order$\"2\",\n       paired=TRUE, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  srchscrl.wide.order$\"1\" and srchscrl.wide.order$\"2\"\nt = -1.3304, df = 19, p-value = 0.1991\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -47.34704  10.54704\nsample estimates:\nmean difference \n          -18.4 \n```\n:::\n:::\n\nWe fail to reject the null hypothesis that the responses do not differ according to order. To phrase this in a more readable (!) way, we have evidence that the order does not matter.\n\n### Running the paired $t$-test\n\nIt now makes sense to use a paired $t$-test since the ANOVA assumptions have been satisfied. This is a parametric test of Time where we pair subjects by technique. Again, we need the wide-format table to conduct a paired test. The wide-format table has one row for each subject rather than one row for each observation.\n\n::: {.cell}\n\n```{.r .cell-code}\nsrchscrl.wide.tech = dcast(srchscrl, Subject ~ Technique,\n\t\t\t   value.var=\"Time\")\n```\n:::\n\n\n```{.r .cell-code}\nxtable(head(srchscrl.wide.tech),\n       caption=\"First rows of wide technique\")\n```\n\n\\begin{table}[ht]\n\\centering\n\\begin{tabular}{rlrr}\n  \\toprule\n & Subject & Scroll & Search \\\\ \n  \\midrule\n1 & 1 & 152.00 & 98.00 \\\\ \n  2 & 2 & 148.00 & 57.00 \\\\ \n  3 & 3 & 160.00 & 86.00 \\\\ \n  4 & 4 & 113.00 & 65.00 \\\\ \n  5 & 5 & 181.00 & 102.00 \\\\ \n  6 & 6 & 70.00 & 126.00 \\\\ \n   \\bottomrule\n\\end{tabular}\n\\caption{First rows of wide technique} \n\\end{table}\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(srchscrl.wide.tech$Search, srchscrl.wide.tech$Scroll,\n       paired=TRUE, var.equal=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  srchscrl.wide.tech$Search and srchscrl.wide.tech$Scroll\nt = -3.6399, df = 19, p-value = 0.001743\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -63.63083 -17.16917\nsample estimates:\nmean difference \n          -40.4 \n```\n:::\n:::\n\nThis supports the intuition we developed doing the histogram and boxplots only now we have a valid statistical test to support this intuition.\n\nSuppose we did not satisfy the ANOVA assumptions.\nThen we would conduct the nonparametric equivalent of paired-samples t-test.\n\n### Exploring a Poisson-distributed factor\n\nExplore the Errors response; error counts are often Poisson-distributed.\n\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(srchscrl, ~ Technique, function(data)\n      summary(data$Errors))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    0       0    0.5  0.7       1    2\n2    Search    1       2    2.5  2.5       3    4\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(srchscrl, ~ Technique, summarise,\n      Errors.mean=mean(Errors), Errors.sd=sd(Errors))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Errors.mean Errors.sd\n1    Scroll         0.7 0.8013147\n2    Search         2.5 1.0513150\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(srchscrl,aes(Errors,fill=Technique)) +\n  geom_histogram(bins=20,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-61-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(srchscrl,aes(Technique,Errors,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-61-2.png){width=672}\n:::\n:::\n\nTry to fit a Poisson distribution for count data. Note that `ks.test()`\nonly works for continuous distributions, but Poisson distributions \nare discrete, so use fitdist, not fitdistr, and test with gofstat.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fitdistrplus)\nfit = fitdist(srchscrl[srchscrl$Technique == \"Search\",]$Errors,\n\t      \"pois\", discrete=TRUE)\ngofstat(fit) # goodness-of-fit test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi-squared statistic:  1.522231 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4671449 \n   the p-value may be wrong with some theoretical counts < 5  \nChi-squared table:\n     obscounts theocounts\n<= 1  4.000000   5.745950\n<= 2  6.000000   5.130312\n<= 3  6.000000   4.275260\n> 3   4.000000   4.848477\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   65.61424\nBayesian Information Criterion   66.60997\n```\n:::\n\n```{.r .cell-code}\nfit = fitdist(srchscrl[srchscrl$Technique == \"Scroll\",]$Errors,\n\t      \"pois\", discrete=TRUE)\ngofstat(fit) # goodness-of-fit test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi-squared statistic:  0.3816087 \nDegree of freedom of the Chi-squared distribution:  1 \nChi-squared p-value:  0.5367435 \n   the p-value may be wrong with some theoretical counts < 5  \nChi-squared table:\n     obscounts theocounts\n<= 0 10.000000   9.931706\n<= 1  6.000000   6.952194\n> 1   4.000000   3.116100\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   45.53208\nBayesian Information Criterion   46.52781\n```\n:::\n:::\n\nConduct a Wilcoxon signed-rank test on Errors.\n::: {.cell}\n\n```{.r .cell-code}\nwilcoxsign_test(Errors ~ Technique | Subject,\n\t\tdata=srchscrl, distribution=\"exact\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n\t stratified by block\nZ = -3.6701, p-value = 6.104e-05\nalternative hypothesis: true mu is not equal to 0\n```\n:::\n:::\n\nNote: the term afer the \"|\" indicates the within-subjects blocking term for matched pairs.\n\n### Examining a Likert scale response item\n\nNow also examine Effort, the ordinal Likert scale response (1-7).\n\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(srchscrl, ~ Technique, function(data)\n      summary(data$Effort))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    1       3      4  4.4    6.00    7\n2    Search    1       3      4  3.6    4.25    5\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(srchscrl, ~ Technique, summarise,\n      Effort.mean=mean(Effort), Effort.sd=sd(Effort))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Effort.mean Effort.sd\n1    Scroll         4.4  1.698296\n2    Search         3.6  1.187656\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(srchscrl,aes(Effort,fill=Technique)) +\n  geom_histogram(bins=20,alpha=0.9,position=position_dodge()) +\n  scale_fill_brewer(palette=\"Blues\") +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-65-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(srchscrl,aes(Technique,Effort,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Set3\") +\n  geom_dotplot(show.legend=FALSE,binaxis='y',stackdir='center',dotsize=1) +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-65-2.png){width=672}\n:::\n:::\n\nOur response is ordinal within-subjects, so use nonparametric Wilcoxon signed-rank.\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcoxsign_test(Effort ~ Technique | Subject,\n\t\tdata=srchscrl, distribution=\"exact\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n\t stratified by block\nZ = 1.746, p-value = 0.08746\nalternative hypothesis: true mu is not equal to 0\n```\n:::\n:::\n\n\n\n## People doing tasks on different phones in different postures (Factorial ANOVA)\n\nThe scenario is text entry on smartphone keyboards: iPhone and Galaxy, in different postures: sitting, walking, standing.\n\nThe statistics employed include\nFactorial ANOVA,\nrepeated measures ANOVA,\nmain effects,\ninteraction effects,\nthe Aligned Rank Transform for nonparametric ANOVAs.\n\nThis is a $3 \\times 2$ mixed factorial design. It is mixed in the sense that there is a between-subjects factor (Keyboard) and a within-subjects factor (Posture).\nIt is balanced in the sense that there are twelve persons using each Keyboard and they are each examined for all three levels of Posture.\n\n### Read and describe the data\n\n::: {.cell}\n\n```{.r .cell-code}\nmbltxt <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/mbltxt.csv\"))\nhead(mbltxt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 6\n  Subject Keyboard Posture Posture_Order   WPM Error_Rate\n    <dbl> <chr>    <chr>           <dbl> <dbl>      <dbl>\n1       1 iPhone   Sit                 1  20.2     0.022 \n2       1 iPhone   Stand               2  23.7     0.03  \n3       1 iPhone   Walk                3  20.8     0.0415\n4       2 iPhone   Sit                 1  20.9     0.022 \n5       2 iPhone   Stand               3  23.3     0.0255\n6       2 iPhone   Walk                2  19.1     0.0355\n```\n:::\n\n```{.r .cell-code}\nmbltxt <- within(mbltxt, Subject <- as.factor(Subject))\nmbltxt <- within(mbltxt, Keyboard <- as.factor(Keyboard))\nmbltxt <- within(mbltxt, Posture <- as.factor(Posture))\nmbltxt <- within(mbltxt, Posture_Order <- as.factor(Posture_Order))\nsummary(mbltxt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject     Keyboard   Posture   Posture_Order      WPM        \n 1      : 3   Galaxy:36   Sit  :24   1:24          Min.   : 9.454  \n 2      : 3   iPhone:36   Stand:24   2:24          1st Qu.:19.091  \n 3      : 3               Walk :24   3:24          Median :21.032  \n 4      : 3                                        Mean   :20.213  \n 5      : 3                                        3rd Qu.:23.476  \n 6      : 3                                        Max.   :25.380  \n (Other):54                                                        \n   Error_Rate     \n Min.   :0.01500  \n 1st Qu.:0.02200  \n Median :0.03050  \n Mean   :0.03381  \n 3rd Qu.:0.04000  \n Max.   :0.06950  \n                  \n```\n:::\n:::\n\n### Explore the WPM (words per minute) data\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- mbltxt |>\n  group_by(Keyboard,Posture) |>\n  summarize(\n    WPM.median=median(WPM),\n    WPM.mean=mean(WPM),\n    WPM.sd=sd(WPM)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'Keyboard'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\ns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n# Groups:   Keyboard [2]\n  Keyboard Posture WPM.median WPM.mean WPM.sd\n  <fct>    <fct>        <dbl>    <dbl>  <dbl>\n1 Galaxy   Sit           23.8     23.9  0.465\n2 Galaxy   Stand         21.2     21.2  0.810\n3 Galaxy   Walk          12.2     12.1  1.26 \n4 iPhone   Sit           20.9     21.0  0.701\n5 iPhone   Stand         23.8     23.9  0.834\n6 iPhone   Walk          19.1     19.2  1.35 \n```\n:::\n:::\n\n### Histograms for both factors\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mbltxt,aes(WPM,fill=Keyboard)) +\n  geom_histogram(bins=20,alpha=0.9,position=\"dodge\",show.legend=FALSE) +\n  scale_color_brewer() +\n  scale_fill_brewer() +\n  facet_grid(Keyboard~Posture) +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-69-1.png){width=672}\n:::\n:::\n\n### Boxplot of both factors\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mbltxt,aes(Keyboard,WPM,fill=Keyboard)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Blues\") +\n  facet_wrap(~Posture) +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-70-1.png){width=672}\n:::\n:::\n\n### An interaction plot\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, WPM,\n                      ylim=c(0, max(mbltxt$WPM))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n### Test for a Posture order effect\n\nThis is to ensure that counterbalancing worked.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ez)\nm <- ezANOVA(dv=WPM,\n            between=Keyboard,\n            within=Posture_Order,\n            wid=Subject,\n            data=mbltxt)\nm$Mauchly\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Effect         W         p p<.05\n3          Posture_Order 0.9912922 0.9122583      \n4 Keyboard:Posture_Order 0.9912922 0.9122583      \n```\n:::\n:::\n\nWikipedia tells us that\n\"Sphericity is an important assumption of a repeated-measures ANOVA. It refers to the condition where the variances of the differences between all possible pairs of within-subject conditions (i.e., levels of the independent variable) are equal. The violation of sphericity occurs when it is not the case that the variances of the differences between all combinations of the conditions are equal. If sphericity is violated, then the variance calculations may be distorted, which would result in an $F$-ratio that would be inflated.\" (from the Wikipedia article on Mauchly's sphericity test)\n\nMauchly's test of sphericity above tells us that there is *not* a significant departure from sphericity, so we can better rely on the $F$-statistic in the following ANOVA, the purpose of which is to detect any order effect that would interfere with our later results.\n\n::: {.cell}\n\n```{.r .cell-code}\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Effect DFn DFd            F            p p<.05          ges\n2               Keyboard   1  22 1.244151e+02 1.596641e-10     * 0.0794723123\n3          Posture_Order   2  44 5.166254e-02 9.497068e-01       0.0023071128\n4 Keyboard:Posture_Order   2  44 2.830819e-03 9.971734e-01       0.0001266932\n```\n:::\n:::\n\nThe $F$-statistic for Posture_Order is very small, indicating that there is not an order effect. That gives us the confidence to run the ANOVA test we wanted to run all along.\n\n## Differences between people's performance and within a person's performance (Two-way mixed factorial ANOVA)\n\nSince a mixed factorial design by definition has both a between-subjects and a within-subjects factor, we don't need to also mention that this is a repeated measures test.\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- ezANOVA(dv=WPM,\n            between=Keyboard,\n            within=Posture,\n            wid=Subject,\n            data=mbltxt)\nm$Mauchly\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Effect         W           p p<.05\n3          Posture 0.6370236 0.008782794     *\n4 Keyboard:Posture 0.6370236 0.008782794     *\n```\n:::\n:::\n\nIn this case, sphericity *is* violated, so we need to additionally apply the Greenhouse-Geisser correction or the less conservative Huyn-Feldt correction. Nevertheless, let's look at the uncorrected ANOVA table. Later, we'll compare it with the uncorrected version provided by the `aov()` function.\n\n::: {.cell}\n\n```{.r .cell-code}\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Effect DFn DFd        F            p p<.05       ges\n2         Keyboard   1  22 124.4151 1.596641e-10     * 0.6151917\n3          Posture   2  44 381.4980 1.602465e-28     * 0.9255880\n4 Keyboard:Posture   2  44 157.1600 9.162076e-21     * 0.8367128\n```\n:::\n:::\n\nNote that \"ges\" in the ANOVA table is the generalized eta-squared measure\nof effect size, $\\eta^2_G$, preferred to eta-squared or partial eta-squared. \nSee Roger Bakeman (2005) \"Recommended effect size statistics for repeated measures designs\", *Behavior Research Methods*, 37 (3) pages 379--384. There, he points out that the usual $\\eta^2$ is the ratio of effect to total variance:\n\n$$\\eta^2=\\frac{SS_{\\text{effect}}}{SS_{\\text{total}}}$$\n\nwhere $SS$ is sum of squares. This is similar to the $R^2$ measure typically reported for regression results. The generalized version is alleged to compensate for the deficiencies that $\\eta^2$ shares with $R^2$, mainly that it can be improved by simply adding more predictors. The generalized version looks like this:\n\n$$\\eta^2_G=\\frac{SS_{\\text{effect}}}{\\delta \\times SS_{\\text{effect}} + \\sum SS_{\\text{measured}}}$$\n\nHere $\\delta=0$ if the effect involves one or more measured factors and $\\delta=1$ if the effect involves only manipulated factors. (Actually it is a little more complicated---here I'm just trying to convey a crude idea that $\\eta^2_G$ ranges between 0 and 1 and that, as it approaches 1, the size of the effect is greater. Oddly enough, it is common to report effect sizes as simply small, medium, or large.)\n\nNow compute the corrected degrees of freedom for each corrected effect.\n\n::: {.cell}\n\n```{.r .cell-code}\npos <- match(m$'Sphericity Corrections'$Effect,\n            m$ANOVA$Effect) # positions of within-Ss efx in m$ANOVA\nm$Sphericity$GGe.DFn <- m$Sphericity$GGe * m$ANOVA$DFn[pos] # Greenhouse-Geisser\nm$Sphericity$GGe.DFd <- m$Sphericity$GGe * m$ANOVA$DFd[pos]\nm$Sphericity$HFe.DFn <- m$Sphericity$HFe * m$ANOVA$DFn[pos] # Huynh-Feldt\nm$Sphericity$HFe.DFd <- m$Sphericity$HFe * m$ANOVA$DFd[pos]\nm$Sphericity\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Effect       GGe        p[GG] p[GG]<.05       HFe        p[HF]\n3          Posture 0.7336884 1.558280e-21         * 0.7731517 1.432947e-22\n4 Keyboard:Posture 0.7336884 7.800756e-16         * 0.7731517 1.447657e-16\n  p[HF]<.05  GGe.DFn  GGe.DFd  HFe.DFn  HFe.DFd\n3         * 1.467377 32.28229 1.546303 34.01868\n4         * 1.467377 32.28229 1.546303 34.01868\n```\n:::\n:::\n\nThe above table shows the Greenhouse Geisser correction to the numerator (GGe.DFn) and denominator (GGe.DFd) degrees of freedom and the resulting $p$-values (p[GG]). The Greenhouse Geiser epsilon statistic ($\\epsilon$) is shown as GGe. There is an analogous set of measures for the less conservative Huynh-Feldt correction. Note that you could calculate a more conservative $F$-statistic using the degrees of freedom given even though a corrected $F$-statistic is not shown for some reason.\n\n## ANOVA results from `aov()`\nThe uncorrected results from the `ez` package are the same as the `aov()` function in base R, shown below.\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- aov(WPM ~ Keyboard * Posture + Error(Subject/Posture),\n        data=mbltxt) # fit model\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nError: Subject\n          Df Sum Sq Mean Sq F value  Pr(>F)    \nKeyboard   1  96.35   96.35   124.4 1.6e-10 ***\nResiduals 22  17.04    0.77                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Subject:Posture\n                 Df Sum Sq Mean Sq F value Pr(>F)    \nPosture           2  749.6   374.8   381.5 <2e-16 ***\nKeyboard:Posture  2  308.8   154.4   157.2 <2e-16 ***\nResiduals        44   43.2     1.0                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n### Manual post hoc pairwise comparisons\nBecause the ANOVA table showed a significant interaction effect and the significance of that interaction effect was borne out by the small p[GG] value, it makes sense to conduct post hoc pairwise comparisons. These require reshaping the data to a wide format because the $t$ test expects data in that format.\n\n::: {.cell}\n\n```{.r .cell-code}\nmbltxt.wide <- dcast(mbltxt, Subject + Keyboard ~ Posture,\n                    value.var=\"WPM\")\nhead(mbltxt.wide)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Subject Keyboard     Sit   Stand    Walk\n1       1   iPhone 20.2145 23.7485 20.7960\n2       2   iPhone 20.8805 23.2595 19.1305\n3       3   iPhone 21.2635 23.4945 20.8545\n4       4   iPhone 20.7080 23.9220 18.2575\n5       5   iPhone 21.0075 23.4700 17.7105\n6       6   iPhone 19.9115 24.2975 19.8550\n```\n:::\n\n```{.r .cell-code}\nsit <- t.test(mbltxt.wide$Sit ~ Keyboard, data=mbltxt.wide)\nstd <- t.test(mbltxt.wide$Stand ~ Keyboard, data=mbltxt.wide)\nwlk <- t.test(mbltxt.wide$Walk ~ Keyboard, data=mbltxt.wide)\np.adjust(c(sit$p.value, std$p.value, wlk$p.value), method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.842490e-10 4.622384e-08 1.450214e-11\n```\n:::\n:::\n\nThe above $p$-values indicate significant differences for all three.\n\n### Compare iPhone 'sit' and 'walk'\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\ntst<-t.test(mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Sit,\n       mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Walk,\n       paired=TRUE)\ntst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\", ]$Sit and mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\", ]$Walk\nt = 3.6259, df = 11, p-value = 0.003985\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.6772808 2.7695525\nsample estimates:\nmean difference \n       1.723417 \n```\n:::\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nboxplot(mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Sit,\n        mbltxt.wide[mbltxt.wide$Keyboard == \"iPhone\",]$Walk,\n        xlab=\"iPhone.Sit vs. iPhone.Walk\", ylab=\"WPM\")\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-79-1.png){width=672}\n:::\n:::\n\n## What if ANOVA assumptions aren't met? (Nonparametric approach to factorial ANOVA)\n\nThe rest of this section concerns a nonparametric approach developed at the University of Washington.\n\n### The Aligned Rank Transform (ART) procedure\n[http://depts.washington.edu/aimgroup/proj/art/](http://depts.washington.edu/aimgroup/proj/art/)\n\n### Explore the Error_Rate data\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- mbltxt |>\n  group_by(Keyboard,Posture) |>\n  summarize(\n    WPM.median=median(Error_Rate),\n    WPM.mean=mean(Error_Rate),\n    WPM.sd=sd(Error_Rate)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'Keyboard'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\ns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n# Groups:   Keyboard [2]\n  Keyboard Posture WPM.median WPM.mean  WPM.sd\n  <fct>    <fct>        <dbl>    <dbl>   <dbl>\n1 Galaxy   Sit         0.019    0.0194 0.00243\n2 Galaxy   Stand       0.0305   0.0307 0.00406\n3 Galaxy   Walk        0.0658   0.0632 0.00575\n4 iPhone   Sit         0.0205   0.0199 0.00248\n5 iPhone   Stand       0.0302   0.0298 0.00258\n6 iPhone   Walk        0.04     0.0399 0.00405\n```\n:::\n:::\n\n### Histograms of Error_Rate\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mbltxt,aes(Error_Rate,fill=Keyboard)) +\n  geom_histogram(bins=20,alpha=0.9,position=\"dodge\",show.legend=FALSE) +\n  scale_color_brewer() +\n  scale_fill_brewer() +\n  facet_grid(Keyboard~Posture) +\n  theme_tufte(base_size=8)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-81-1.png){width=672}\n:::\n:::\n\n### Box plots of Error_Rate\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mbltxt,aes(Keyboard,Error_Rate,fill=Keyboard)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Greens\") +\n  facet_wrap(~Posture) +\n  theme_tufte(base_size=7)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-82-1.png){width=672}\n:::\n:::\n\n### Interaction plot of Error_Rate\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, Error_Rate,\n                      ylim=c(0, max(mbltxt$Error_Rate))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-83-1.png){width=672}\n:::\n:::\n\n### Aligned Rank Transform on Error_Rate\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ARTool) # for art, artlm\nm <- art(Error_Rate ~ Keyboard * Posture + (1|Subject), data=mbltxt) # uses LMM\nanova(m) # report anova\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance of Aligned Rank Transformed Data\n\nTable Type: Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df) \nModel: Mixed Effects (lmer)\nResponse: art(Error_Rate)\n\n                         F Df Df.res     Pr(>F)    \n1 Keyboard          89.450  1     22 3.2959e-09 ***\n2 Posture          274.704  2     44 < 2.22e-16 ***\n3 Keyboard:Posture  78.545  2     44 3.0298e-15 ***\n---\nSignif. codes:   0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n```\n:::\n:::\n\n### Examine the normality assumption\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nshapiro.test(residuals(m)) # normality?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(m)\nW = 0.98453, p-value = 0.5227\n```\n:::\n\n```{.r .cell-code}\nqqnorm(residuals(m)); qqline(residuals(m)) # seems to conform\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-85-1.png){width=672}\n:::\n:::\n\n### Interaction plot\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(mbltxt,\n     interaction.plot(Posture, Keyboard, Error_Rate,\n\t\t      ylim=c(0, max(mbltxt$Error_Rate)))) # for convenience\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-86-1.png){width=672}\n:::\n:::\n\n### Conduct post hoc pairwise comparisons within each factor\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(emmeans) # instead of lsmeans\n#. for backward compatibility, emmeans provides an lsmeans() function\nlsmeans(artlm(m, \"Keyboard\"), pairwise ~ Keyboard)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNOTE: Results may be misleading due to involvement in interactions\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n$lsmeans\n Keyboard lsmean   SE df lower.CL upper.CL\n Galaxy     52.3 2.36 22     47.4     57.2\n iPhone     20.7 2.36 22     15.8     25.6\n\nResults are averaged over the levels of: Posture \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n$contrasts\n contrast        estimate   SE df t.ratio p.value\n Galaxy - iPhone     31.6 3.34 22   9.458  <.0001\n\nResults are averaged over the levels of: Posture \nDegrees-of-freedom method: kenward-roger \n```\n:::\n\n```{.r .cell-code}\nlsmeans(artlm(m, \"Posture\"), pairwise ~ Posture)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNOTE: Results may be misleading due to involvement in interactions\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n$lsmeans\n Posture lsmean   SE   df lower.CL upper.CL\n Sit       12.5 1.47 65.9     9.57     15.4\n Stand     36.5 1.47 65.9    33.57     39.4\n Walk      60.5 1.47 65.9    57.57     63.4\n\nResults are averaged over the levels of: Keyboard \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n$contrasts\n contrast     estimate   SE df t.ratio p.value\n Sit - Stand       -24 2.05 44 -11.720  <.0001\n Sit - Walk        -48 2.05 44 -23.439  <.0001\n Stand - Walk      -24 2.05 44 -11.720  <.0001\n\nResults are averaged over the levels of: Keyboard \nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 3 estimates \n```\n:::\n\n```{.r .cell-code}\n#. Warning: don't do the following in ART!\n#lsmeans(artlm(m, \"Keyboard : Posture\"), pairwise ~ Keyboard : Posture)\n```\n:::\n\nThe above contrast-testing method is invalid for cross-factor pairwise comparisons in ART.\nand you can't just grab aligned-ranks for manual $t$-tests. instead, use `testInteractions()` \nfrom the `phia` package to perform \"interaction contrasts.\" See `vignette(\"art-contrasts\")`.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(phia)\ntestInteractions(artlm(m, \"Keyboard:Posture\"),\n                 pairwise=c(\"Keyboard\", \"Posture\"), adjustment=\"holm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nChisq Test: \nP-value adjustment method: holm\n                             Value Df    Chisq Pr(>Chisq)    \nGalaxy-iPhone :  Sit-Stand  -5.083  1   0.5584     0.4549    \nGalaxy-iPhone :   Sit-Walk -76.250  1 125.6340     <2e-16 ***\nGalaxy-iPhone : Stand-Walk -71.167  1 109.4412     <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nIn the output, A-B : C-D is interpreted as a difference-of-differences, i.e., the difference \nbetween (A-B | C) and (A-B | D). In words, is the difference between A and B significantly \ndifferent in condition C from condition D?\n\n\n\n## Experiments with interaction effects\n\nThis section reports on three experiments with possible interaction effects: Avatars, Notes, and Social media value. To work through the questions, you need the three `csv` files containing the data: `avatars.csv`, `notes.csv`, and `socialvalue.csv`.\n\nThese experiments may be between-subjects, within-subjects, or mixed. To be a mixed factorial design, there would have to be at least two independent variables and at least one within-subjects factor and at least one between-subjects factor.\n\n### Sentiments about Avatars among males and females (Interaction effects)\n\nThirty males and thirty females were shown an avatar that was either male or female and asked to write a story about that avatar. The number of positive sentiments in the story were summed. What kind of experimental design is this? [Answer: It is a $2\\times 2$ between-subjects design with factors for Sex (M, F) and Avatar (M, F).]\n\n::: {.cell}\n\n```{.r .cell-code}\navatars <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/avatars.csv\"))\navatars$Subject <- factor(avatars$Subject)\nsummary(avatars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject       Sex               Avatar            Positives    \n 1      : 1   Length:60          Length:60          Min.   : 32.0  \n 2      : 1   Class :character   Class :character   1st Qu.: 65.0  \n 3      : 1   Mode  :character   Mode  :character   Median : 84.0  \n 4      : 1                                         Mean   : 85.1  \n 5      : 1                                         3rd Qu.:104.2  \n 6      : 1                                         Max.   :149.0  \n (Other):54                                                        \n```\n:::\n:::\n\nWhat's the average number of positive sentiments for the most positive combination of Sex and Avatar?\n\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(avatars,~Sex*Avatar,summarize,\n      Pos.mean=mean(Positives),\n      Pos.sd=sd(Positives))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Sex Avatar  Pos.mean   Pos.sd\n1 Female Female  63.13333 17.48414\n2 Female   Male  85.20000 25.31008\n3   Male Female 100.73333 18.72152\n4   Male   Male  91.33333 19.66384\n```\n:::\n:::\n\nCreate an interaction plot with Sex on the X-Axis and Avatar as the traces. Do the lines cross? Do the same for reversed axes.\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(avatars,interaction.plot(Sex,Avatar,Positives,\n\t\t\t      ylim=c(0,max(avatars$Positives))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-91-1.png){width=672}\n:::\n\n```{.r .cell-code}\nwith(avatars,interaction.plot(Avatar,Sex,Positives,\n\t\t\t      ylim=c(0,max(avatars$Positives))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-91-2.png){width=672}\n:::\n:::\n\nConduct a factorial ANOVA on Positives by Sex and Avatar and report the largest $F$-statistic. Report which effects are significant.\n\n::: {.cell}\n\n```{.r .cell-code}\nm<-ezANOVA(dv=Positives,between=c(Sex,Avatar),\n\t   wid=Subject,data=avatars)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Converting \"Sex\" to factor for ANOVA.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Converting \"Avatar\" to factor for ANOVA.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nCoefficient covariances computed by hccm()\n```\n:::\n\n```{.r .cell-code}\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Effect DFn DFd         F            p p<.05        ges\n1        Sex   1  56 17.041756 0.0001228287     * 0.23331526\n2     Avatar   1  56  1.429598 0.2368686270       0.02489305\n3 Sex:Avatar   1  56  8.822480 0.0043757511     * 0.13610216\n```\n:::\n:::\n\nConduct planned pairwise comparisons using independent-samples $t$-tests. Ask whether females produced different numbers of positive sentiments for male vs female avatars. Then ask whether males did the same. Assume equal variances and use Holm's sequential Bonferroni procedure to correct for multiple comparisons.\n\n::: {.cell}\n\n```{.r .cell-code}\nf<-t.test(avatars[avatars$Sex==\"Female\" & avatars$Avatar==\"Male\",]$Positives,\n\t  avatars[avatars$Sex==\"Female\" & avatars$Avatar==\"Female\",]$Positives,\n\t  var.equal=TRUE)\nf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  avatars[avatars$Sex == \"Female\" & avatars$Avatar == \"Male\", ]$Positives and avatars[avatars$Sex == \"Female\" & avatars$Avatar == \"Female\", ]$Positives\nt = 2.7782, df = 28, p-value = 0.009647\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  5.796801 38.336533\nsample estimates:\nmean of x mean of y \n 85.20000  63.13333 \n```\n:::\n\n```{.r .cell-code}\nm<-t.test(avatars[avatars$Sex==\"Male\" & avatars$Avatar==\"Male\",]$Positives,\n\t  avatars[avatars$Sex==\"Male\" & avatars$Avatar==\"Female\",]$Positives,\n\t  var.equal=TRUE)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  avatars[avatars$Sex == \"Male\" & avatars$Avatar == \"Male\", ]$Positives and avatars[avatars$Sex == \"Male\" & avatars$Avatar == \"Female\", ]$Positives\nt = -1.3409, df = 28, p-value = 0.1907\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23.759922   4.959922\nsample estimates:\nmean of x mean of y \n 91.33333 100.73333 \n```\n:::\n\n```{.r .cell-code}\np.adjust(c(f$p.value,m$p.value),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01929438 0.19073468\n```\n:::\n:::\n\n### Writing notes with builtin or addon apps on two phones (mixed factorial design)\n\nThe `notes.csv` file describes a study in which iPhone and Android owners used a built-in note-taking app then a third-party note-taking app or vice versa. What kind of experimental design is this? (Answer: A $2 \\times 2$ mixed factorial design with a between-subjects factor for Phone (iPhone, Android) and a within-subjects factor for Notes (Built-in, Add-on).)\n\n::: {.cell}\n\n```{.r .cell-code}\nnotes <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/notes.csv\"))\nnotes$Subject<-factor(notes$Subject)\nnotes$Order<-factor(notes$Order)\nsummary(notes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject      Phone              Notes           Order      Words      \n 1      : 2   Length:40          Length:40          1:20   Min.   :259.0  \n 2      : 2   Class :character   Class :character   2:20   1st Qu.:421.8  \n 3      : 2   Mode  :character   Mode  :character          Median :457.0  \n 4      : 2                                                Mean   :459.2  \n 5      : 2                                                3rd Qu.:518.5  \n 6      : 2                                                Max.   :598.0  \n (Other):28                                                               \n```\n:::\n:::\n\nWhat's the average number of words recorded for the most heavily used combination of Phone and Notes?\n\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(notes, ~Phone*Notes,summarize,\n\t     Words.mean=mean(Words),Words.sd=sd(Words))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Phone    Notes Words.mean Words.sd\n1 Android   Add-on      388.1 42.38828\n2 Android Built-in      410.9 77.49043\n3  iPhone   Add-on      504.2 47.29529\n4  iPhone Built-in      533.7 48.04176\n```\n:::\n:::\n\nCreate an interaction plot with Phone on the X-Axis and Notes as the traces. Do the lines cross? Do the same for reversed axes.\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(notes,interaction.plot(Phone,Notes,Words,\n\t\t\t    ylim=c(0,max(notes$Words))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-96-1.png){width=672}\n:::\n\n```{.r .cell-code}\nwith(notes,interaction.plot(Notes,Phone,Words,\n\t\t\t    ylim=c(0,max(notes$Words))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-96-2.png){width=672}\n:::\n:::\n\nTest for an order effect in the presentation of order of the Notes factor. Report the $p$-value.\n\n::: {.cell}\n\n```{.r .cell-code}\nm<-ezANOVA(dv=Words,between=Phone,within=Order,wid=Subject,data=notes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Converting \"Phone\" to factor for ANOVA.\n```\n:::\n\n```{.r .cell-code}\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Effect DFn DFd          F            p p<.05        ges\n2       Phone   1  18 43.5625695 3.375888e-06     * 0.56875437\n3       Order   1  18  0.5486763 4.684126e-01       0.01368098\n4 Phone:Order   1  18  3.0643695 9.705122e-02       0.07189858\n```\n:::\n:::\n\nConduct a factorial ANOVA on Words by Phone and Notes. Report the largest $F$-statistic.\n\n::: {.cell}\n\n```{.r .cell-code}\nm<-ezANOVA(dv=Words,between=Phone,within=Notes,wid=Subject,data=notes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Converting \"Notes\" to factor for ANOVA.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Converting \"Phone\" to factor for ANOVA.\n```\n:::\n\n```{.r .cell-code}\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Effect DFn DFd           F            p p<.05         ges\n2       Phone   1  18 43.56256949 3.375888e-06     * 0.562185697\n3       Notes   1  18  2.35976941 1.418921e-01       0.057972811\n4 Phone:Notes   1  18  0.03872717 8.461951e-01       0.001008948\n```\n:::\n:::\n\nConduct paired-samples $t$-tests to answer two questions. First, did iPhone user enter different numbers of words using the built-in notes app versus the add-on notes app? Second, same for Android. Assume equal variances and use Holm's sequential Bonferroni procedure to correct for multiple comparisons. Report the lowest adjusted $p$-value.\n\n::: {.cell}\n\n```{.r .cell-code}\nnotes.wide<-dcast(notes,Subject+Phone~Notes,value.var=\"Words\")\nhead(notes.wide)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Subject   Phone Add-on Built-in\n1       1  iPhone    464      561\n2       2 Android    433      428\n3       3  iPhone    598      586\n4       4 Android    347      448\n5       5  iPhone    478      543\n6       6 Android    365      445\n```\n:::\n\n```{.r .cell-code}\ni<-t.test(notes.wide[notes.wide$Phone==\"iPhone\",]$'Add-on',\n\t  notes.wide[notes.wide$Phone==\"iPhone\",]$'Built-in',\n\t  paired=TRUE,var.equal=TRUE)\ni\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  notes.wide[notes.wide$Phone == \"iPhone\", ]$\"Add-on\" and notes.wide[notes.wide$Phone == \"iPhone\", ]$\"Built-in\"\nt = -1.8456, df = 9, p-value = 0.09804\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -65.658758   6.658758\nsample estimates:\nmean difference \n          -29.5 \n```\n:::\n\n```{.r .cell-code}\na<-t.test(notes.wide[notes.wide$Phone==\"Android\",]$'Add-on',\n\t  notes.wide[notes.wide$Phone==\"Android\",]$'Built-in',\n\t  paired=TRUE,var.equal=TRUE)\na\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  notes.wide[notes.wide$Phone == \"Android\", ]$\"Add-on\" and notes.wide[notes.wide$Phone == \"Android\", ]$\"Built-in\"\nt = -0.75847, df = 9, p-value = 0.4676\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -90.80181  45.20181\nsample estimates:\nmean difference \n          -22.8 \n```\n:::\n\n```{.r .cell-code}\np.adjust(c(i$p.value,a$p.value),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1960779 0.4675674\n```\n:::\n:::\n\n### Social media value judged by people after watching clips (two-by-two within subject design)\n\nThe file socialvalue.csv describes a study of people viewing a pos or neg film clip then going onto social media and judging the value of the first 100 posts they see. The number of valued posts was recorded. What kind of experimental design is this? [Answer: A $2\\times 2$ within-subject design with factors for Clip (positive, negative) and Social (Facebook, Twitter).]\n\n::: {.cell}\n\n```{.r .cell-code}\nsv <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/socialvalue.csv\"))\nsv$Subject<-factor(sv$Subject)\nsv$Clip<-factor(sv$Clip)\nsv$Social<-factor(sv$Social)\nsv$ClipOrder<-factor(sv$ClipOrder)\nsv$SocialOrder<-factor(sv$SocialOrder)\nsummary(sv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject         Clip    ClipOrder      Social   SocialOrder     Valued    \n 1      : 4   negative:32   1:32      Facebook:32   1:32        Min.   :13.0  \n 2      : 4   positive:32   2:32      Twitter :32   2:32        1st Qu.:52.0  \n 3      : 4                                                     Median :56.0  \n 4      : 4                                                     Mean   :57.3  \n 5      : 4                                                     3rd Qu.:67.0  \n 6      : 4                                                     Max.   :95.0  \n (Other):40                                                                   \n```\n:::\n:::\n\nWhat's the average number of valued posts for the most valued combination of Clip and Social?\n\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(sv, ~Clip*Social,summarize, Valued.mean=mean(Valued),\n      Valued.sd=sd(Valued))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Clip   Social Valued.mean Valued.sd\n1 negative Facebook     46.3125 22.285178\n2 negative  Twitter     55.5625  5.032809\n3 positive Facebook     68.7500 21.151832\n4 positive  Twitter     58.5625  5.656486\n```\n:::\n:::\n\nCreate an interaction plot with Social on the $X$-Axis and Clip as the traces. Do the lines cross? Do the same for reversed axes.\n\n::: {.cell fig.margin='false'}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(sv,interaction.plot(Social,Clip,Valued,\n\t\t\t ylim=c(0,max(sv$Valued))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-102-1.png){width=672}\n:::\n\n```{.r .cell-code}\nwith(sv,interaction.plot(Clip,Social,Valued,\n\t\t\t ylim=c(0,max(sv$Valued))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-102-2.png){width=672}\n:::\n:::\n\nTest for an order effect in the presentation of order of the ClipOrder or SocialOrder factor. Report the $p$-values.\n\n::: {.cell}\n\n```{.r .cell-code}\nm<-ezANOVA(dv=Valued,within=c(ClipOrder,SocialOrder),wid=Subject,data=sv)\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Effect DFn DFd          F         p p<.05          ges\n2             ClipOrder   1  15 0.93707354 0.3483818       0.0253831509\n3           SocialOrder   1  15 0.81236528 0.3816660       0.0143842018\n4 ClipOrder:SocialOrder   1  15 0.01466581 0.9052172       0.0001913088\n```\n:::\n:::\n\nConduct a factorial ANOVA on Valued by Clip and Social. Report the largest $F$-statistic.\n\n::: {.cell}\n\n```{.r .cell-code}\nm<-ezANOVA(dv=Valued,within=c(Clip,Social),wid=Subject,data=sv)\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Effect DFn DFd          F          p p<.05          ges\n2        Clip   1  15 6.99533219 0.01837880     * 0.1469889054\n3      Social   1  15 0.01466581 0.90521722       0.0002340033\n4 Clip:Social   1  15 6.11355984 0.02586779     * 0.0914169000\n```\n:::\n:::\n\nConduct paired-samples $t$-tests to answer two questions. First, on Facebook, were the number of valued posts different after watching a positive or negative clip. Second, same on Twitter.\nAssume equal variances and use Holm's sequential Bonferroni procedure to correct for multiple comparisons. Report the lowest adjusted $p$-value.\n\n::: {.cell}\n\n```{.r .cell-code}\nsv.wide<-dcast(sv,Subject+Social~Clip,value.var=\"Valued\")\nhead(sv.wide)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Subject   Social negative positive\n1       1 Facebook       38       85\n2       1  Twitter       52       53\n3       2 Facebook       73       25\n4       2  Twitter       52       54\n5       3 Facebook       25       95\n6       3  Twitter       53       70\n```\n:::\n\n```{.r .cell-code}\nf<-t.test(sv.wide[sv.wide$Social==\"Facebook\",]$positive,\n\t  sv.wide[sv.wide$Social==\"Facebook\",]$negative,\n\t  paired=TRUE,var.equal=TRUE)\nf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  sv.wide[sv.wide$Social == \"Facebook\", ]$positive and sv.wide[sv.wide$Social == \"Facebook\", ]$negative\nt = 2.5929, df = 15, p-value = 0.02039\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  3.993 40.882\nsample estimates:\nmean difference \n        22.4375 \n```\n:::\n\n```{.r .cell-code}\nt<-t.test(sv.wide[sv.wide$Social==\"Twitter\",]$positive,\n\t  sv.wide[sv.wide$Social==\"Twitter\",]$negative,\n\t  paired=TRUE,var.equal=TRUE)\nt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  sv.wide[sv.wide$Social == \"Twitter\", ]$positive and sv.wide[sv.wide$Social == \"Twitter\", ]$negative\nt = 1.9926, df = 15, p-value = 0.06482\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2089939  6.2089939\nsample estimates:\nmean difference \n              3 \n```\n:::\n\n```{.r .cell-code}\np.adjust(c(f$p.value,t$p.value),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04077153 0.06482275\n```\n:::\n:::\n\nConduct a nonparametric Aligned Rank Transform Procedure on Valued by Clip and Social.\n\n::: {.cell}\n\n```{.r .cell-code}\nm<-art(Valued~Clip*Social+(1|Subject),data=sv)\nanova(m)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance of Aligned Rank Transformed Data\n\nTable Type: Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df) \nModel: Mixed Effects (lmer)\nResponse: art(Valued)\n\n                     F Df Df.res     Pr(>F)    \n1 Clip        17.13224  1     45 0.00015089 ***\n2 Social       0.49281  1     45 0.48629341    \n3 Clip:Social 11.31751  1     45 0.00157736  **\n---\nSignif. codes:   0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n```\n:::\n:::\n\nConduct interaction contrasts to discover whether the difference on Facebook was itself different from the difference on Twitter. Report the $\\chi^2$ statistic.\n\n::: {.cell}\n\n```{.r .cell-code}\ntestInteractions(artlm(m,\"Clip:Social\"),\n\t\t pairwise=c(\"Clip\",\"Social\"),adjustment=\"holm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nChisq Test: \nP-value adjustment method: holm\n                                      Value Df  Chisq Pr(>Chisq)    \nnegative-positive : Facebook-Twitter -29.25  1 11.318  0.0007678 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\n## What if errors are not normally distributed? (Generalized linear models)\n\nHere are three examples of generalized linear models. The first is analyzed using nominal logistic regression, the second is analyzed via ordinal logistic regression, and the third is analyzed via Poisson regression.\n\nAs Wikipedia tells us, a generalized linear model or GLM is a flexible generalization of ordinary linear regression that allows for response variables with error distribution models other than a normal distribution.\nThere is also something called a *general* linear model but it is not the same thing as a *generalized* linear model. It is just the general form of the ordinary linear regression model: $\\mathbfit{Y=X\\beta+\\epsilon}$.\n\nGLMs that we examine here are good for between-subjects studies so we'll actually recode one of our fictitious data sets to be between subjects just to have an example to use.\n\n### Preferences among websites by males and females (GLM 1: Nominal logistic regression for preference responses)\n\n###  Multinomial distribution with logit link function\n\nThe `prefsABCsex.csv` file records preferences among three websites A, B, and C expressed by males and females. The subject number, preference and sex were recorded.\n\nThe logit link function is the log odds function, generally $\\text{logit}(p)=\\ln \\frac{p}{1-p}$, where $p$ is the probability of an event such as choosing website A. The form of the link function is $\\mathbfit{X\\beta}=\\ln\\frac{\\mu}{1-\\mu}$. This is just the relationship of a matrix of predictors times a vector of parameters $\\mathbfit{\\beta}$ to the logit of the mean of the distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\nprefsABCsex <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/prefsABCsex.csv\"))\nhead(prefsABCsex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  Subject Pref  Sex  \n    <dbl> <chr> <chr>\n1       1 C     F    \n2       2 C     M    \n3       3 B     M    \n4       4 C     M    \n5       5 C     M    \n6       6 B     F    \n```\n:::\n\n```{.r .cell-code}\nprefsABCsex$Subject<-factor(prefsABCsex$Subject)\nprefsABCsex$Sex<-factor(prefsABCsex$Sex)\nsummary(prefsABCsex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject       Pref           Sex   \n 1      : 1   Length:60          F:29  \n 2      : 1   Class :character   M:31  \n 3      : 1   Mode  :character         \n 4      : 1                            \n 5      : 1                            \n 6      : 1                            \n (Other):54                            \n```\n:::\n\n```{.r .cell-code}\nggplot(prefsABCsex[prefsABCsex$Sex == \"M\",],aes(Pref)) +\n  theme_tufte() +\n  geom_bar(width=0.25,fill=\"gray\") +\n  geom_hline(yintercept=seq(0, 20, 5), col=\"white\", lwd=1) +\n  annotate(\"text\", x = 1.5, y = 18, adj=1,  family=\"serif\",\n    label = c(\"Males prefer\\nwebsite C\"))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-108-1.png){width=75%}\n:::\n\n```{.r .cell-code}\nggplot(prefsABCsex[prefsABCsex$Sex == \"F\",],aes(Pref)) +\n  theme_tufte() +\n  geom_bar(width=0.25,fill=\"gray\") +\n  geom_hline(yintercept=seq(0, 20, 5), col=\"white\", lwd=1) +\n  annotate(\"text\", x = 1.5, y = 18, adj=1,  family=\"serif\",\n    label = c(\"Females dislike\\nwebsite A\"))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-108-2.png){width=75%}\n:::\n:::\n\nThese histograms lead us to suspect that C is preferred by males and that A is disliked by females, but we should still run tests to be convinced that the variability observed is not due to chance.\n\nAnalyze Pref by Sex using multinomial logistic regression, aka nominal logistic regression. Here we are testing for whether there is a difference between the sexes regarding their preferences.\n\nThe annotation `type=3` is borrowed from SAS and refers to one of three ways of handling an unbalanced design. This experimental design is unbalanced because there are more males than females being tested. This way of handling the unbalanced design is only valid if there are significant interactions, as hinted by the gross differences between the preceding histograms.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet) # provides multinom()\n#. library(car) # provides Anova()\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(prefsABCsex$Sex) <- \"contr.sum\"\nm<-multinom(Pref~Sex, data=prefsABCsex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  9 (4 variable)\ninitial  value 65.916737 \niter  10 value 55.099353\niter  10 value 55.099353\nfinal  value 55.099353 \nconverged\n```\n:::\n\n```{.r .cell-code}\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n    LR Chisq Df Pr(>Chisq)  \nSex   7.0744  2    0.02909 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nThe Analysis of Deviance table tells us that there is a significant main effect for Sex. It does not tell us more detail but motivates pairwise tests to get more detail. If there were no significant effect, pairwise tests would not be warranted.\n\nPairwise tests tell which of the bins are over or under populated based on the assumption that each bin should contain one third of the observations (hence `p=1/3`). When making multiple comparisons we would overstate the significance of the differences so we use Holm's sequential Bonferroni procedure to correct this.\n\n::: {.cell}\n\n```{.r .cell-code}\nma<-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"A\"),\n\t       nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmb<-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"B\"),\n\t       nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\nmc<-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"M\",]$Pref == \"C\"),\n\t       nrow(prefsABCsex[prefsABCsex$Sex == \"M\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(ma$p.value, mb$p.value, mc$p.value), method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.109473564 0.126622172 0.001296754\n```\n:::\n\n```{.r .cell-code}\nfa<-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"A\"),\n\t       nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfb<-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"B\"),\n\t       nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\nfc<-binom.test(sum(prefsABCsex[prefsABCsex$Sex == \"F\",]$Pref == \"C\"),\n\t       nrow(prefsABCsex[prefsABCsex$Sex == \"F\",]), p=1/3)\n#. correct for multiple comparisons\np.adjust(c(fa$p.value, fb$p.value, fc$p.value), method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02703274 0.09447821 0.69396951\n```\n:::\n:::\n\nThe preceding tests confirm what we suspected from looking at histograms: males prefer C and females dislike A. We see this by looking at the adjusted $p$-values, where the first row, third value is significant and the second row, first value is significant.\n\nHow would we write this up in a report? We could make the following claim. We tested the main effect for sex and found a significant result, $\\chi^2_2=7.1, p<0.05$. An exact binomial test found the preference among males for website C greater than chance, $p<0.01$. An exact binomial test found the preference among females against website A greater than chance, $p<0.05$. No other significant differences were found.\n\n### Judgments of perceived effort (GLM 2: Ordinal logistic regression for Likert responses)\n\n### Multinomial distribution with cumulative logit link function\nIn this example, users are either searching, scrolling or using voice to find contacts in a smartphone address book. The time it takes to find a certain number of contacts, the perceived effort, and the number of errors are all recorded. Of interest now is the perceived effort, recorded on a Likert scale. A Likert scale can not be normally distributed because of the restrictions on the ends and is not likely to even look vaguely normal.\n\nThe cumulative logit link function is like the logit link function:\n\n$$\\text{logit}(P(Y\\leqslant j|x))=\\ln\\frac{P(Y\\leqslant j|x)}{1-P(Y\\leqslant j|x)} \\text{ where }Y=1,2,\\ldots,J$$\n\nIn this case $J$ ranges from 1 to 7.\n\nRead in the data and examine it. We see that it is a within-subjects study but it is a fictitious study anyway so we will recode it as if it were a between-subjects study. Then we will be able to apply the following techniques, which we would have to modify for a within-subjects study.\n\n::: {.cell}\n\n```{.r .cell-code}\nsrchscrlvce <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/srchscrlvce.csv\"))\nhead(srchscrlvce)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 6\n  Subject Technique Order  Time Errors Effort\n    <dbl> <chr>     <dbl> <dbl>  <dbl>  <dbl>\n1       1 Search        1    98      4      5\n2       1 Scroll        2   152      0      6\n3       2 Search        2    57      2      2\n4       2 Scroll        1   148      0      3\n5       3 Search        1    86      3      2\n6       3 Scroll        2   160      0      4\n```\n:::\n\n```{.r .cell-code}\nsrchscrlvce$Subject<-(1:nrow(srchscrlvce)) # recode as between-subjects\nsrchscrlvce$Subject<-factor(srchscrlvce$Subject)\nsrchscrlvce$Technique<-factor(srchscrlvce$Technique)\nsrchscrlvce$Order<-NULL # drop order, n/a for between-subjects\nhead(srchscrlvce) # verify\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  Subject Technique  Time Errors Effort\n  <fct>   <fct>     <dbl>  <dbl>  <dbl>\n1 1       Search       98      4      5\n2 2       Scroll      152      0      6\n3 3       Search       57      2      2\n4 4       Scroll      148      0      3\n5 5       Search       86      3      2\n6 6       Scroll      160      0      4\n```\n:::\n\n```{.r .cell-code}\nsummary(srchscrlvce)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject    Technique       Time           Errors         Effort    \n 1      : 1   Scroll:20   Min.   : 49.0   Min.   :0.00   Min.   :1.00  \n 2      : 1   Search:20   1st Qu.: 86.0   1st Qu.:1.00   1st Qu.:3.00  \n 3      : 1   Voice :20   Median : 97.0   Median :2.00   Median :4.00  \n 4      : 1               Mean   :106.2   Mean   :2.75   Mean   :4.15  \n 5      : 1               3rd Qu.:128.0   3rd Qu.:4.00   3rd Qu.:5.00  \n 6      : 1               Max.   :192.0   Max.   :9.00   Max.   :7.00  \n (Other):54                                                            \n```\n:::\n:::\n\nA good description of Effort is the median and quantiles. Another good description is the mean and standard deviation.\n\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(srchscrlvce, ~ Technique,\n       function(data) summary(data$Effort))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    1    3.00      4 4.40    6.00    7\n2    Search    1    3.00      4 3.60    4.25    5\n3     Voice    1    3.75      5 4.45    5.25    6\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(srchscrlvce, ~ Technique,\n       summarize, Effort.mean=mean(Effort), Effort.sd=sd(Effort))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Effort.mean Effort.sd\n1    Scroll        4.40  1.698296\n2    Search        3.60  1.187656\n3     Voice        4.45  1.356272\n```\n:::\n\n```{.r .cell-code}\npar(cex=0.6)\nggplot(srchscrlvce,aes(Effort,fill=Technique)) +\n  geom_histogram(bins=7,alpha=0.8,position=\"dodge\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte()\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-112-1.png){width=75%}\n:::\n\n```{.r .cell-code}\nggplot(srchscrlvce,aes(Technique,Effort,fill=Technique)) +\n  geom_tufteboxplot(show.legend=FALSE) +\n  theme_tufte()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The following aesthetics were dropped during statistical transformation: y\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n```\n:::\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-112-2.png){width=75%}\n:::\n:::\n\nThe boxplots (these are Tufte-style boxplots) are not encouraging. We may not find a significant difference among these three techniques but let us try anyway.\nWe analyze Effort Likert ratings by Technique using ordinal logistic regression.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(MASS) # provides polr()\n#. library(car) # provides Anova()\nsrchscrlvce$Effort <- ordered(srchscrlvce$Effort)\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(srchscrlvce$Technique) <- \"contr.sum\"\nm <- polr(Effort ~ Technique, data=srchscrlvce, Hess=TRUE) # ordinal logistic\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Effort\n          LR Chisq Df Pr(>Chisq)\nTechnique   4.5246  2     0.1041\n```\n:::\n:::\n\nPost hoc pairwise comparisons are NOT justified due to lack of significance\nbut here's how we would do them, just for completeness.\n`Tukey` means to compare all pairs and `holm` is the adjustment due to the double-counting that overstates the significance.\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glht(m, mcp(Technique=\"Tukey\")), test=adjusted(type=\"holm\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: polr(formula = Effort ~ Technique, data = srchscrlvce, Hess = TRUE)\n\nLinear Hypotheses:\n                      Estimate Std. Error z value Pr(>|z|)\nSearch - Scroll == 0 -1.016610   0.584614  -1.739    0.191\nVoice - Scroll == 0   0.007397   0.587700   0.013    0.990\nVoice - Search == 0   1.024007   0.552298   1.854    0.191\n(Adjusted p values reported -- holm method)\n```\n:::\n:::\n\nHow would we express this in a report? We would simply say that we found no significant differences between the three techniques.\n\n### Counting errors in a task (GLM 3: Poisson regression for count responses)\n\n### Poisson distribution with log link function\n\nUsing the same data but now focus on the Errors column instead of effort. Errors likely have a Poisson distribution. The log link function is just $\\mathbfit{X\\beta}=\\ln(\\mu)$ rather than the more elaborate logit link function we saw before.\n\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(srchscrlvce, ~ Technique,\n             function(data) summary(data$Errors))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Min. 1st Qu. Median Mean 3rd Qu. Max.\n1    Scroll    0    0.00    0.5 0.70    1.00    2\n2    Search    1    2.00    2.5 2.50    3.00    4\n3     Voice    2    3.75    5.0 5.05    6.25    9\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(srchscrlvce, ~ Technique, summarize,\n             Errors.mean=mean(Errors), Errors.sd=sd(Errors))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Technique Errors.mean Errors.sd\n1    Scroll        0.70 0.8013147\n2    Search        2.50 1.0513150\n3     Voice        5.05 1.9049796\n```\n:::\n\n```{.r .cell-code}\npar(cex=0.6)\nggplot(srchscrlvce,aes(Errors,fill=Technique)) +\n  geom_histogram(bins=9,alpha=0.8,position=\"dodge\") +\n  scale_color_grey() +\n  scale_fill_grey() +\n  theme_tufte()\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-115-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(srchscrlvce,aes(Technique,Errors,fill=Technique)) +\n  geom_boxplot(show.legend=FALSE) +\n  scale_fill_brewer(palette=\"Greens\") +\n  theme_tufte()\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-115-2.png){width=672}\n:::\n:::\n\nThese boxplots are very encouraging. There appears to be a clear difference between all three of these techniques. Notice that you could draw horizontal lines across the plot without intersecting the boxes. That represents a high degree of separation.\n\nNow verify that these data are Poisson-distributed with a goodness-of-fit test for each technique. If the results are *not* significant, we expect that the data do not deviate significantly from what we would expect of a Poisson distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(fitdistrplus)\nfit<-fitdist(srchscrlvce[srchscrlvce$Technique == \"Search\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi-squared statistic:  1.522231 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4671449 \n   the p-value may be wrong with some theoretical counts < 5  \nChi-squared table:\n     obscounts theocounts\n<= 1  4.000000   5.745950\n<= 2  6.000000   5.130312\n<= 3  6.000000   4.275260\n> 3   4.000000   4.848477\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   65.61424\nBayesian Information Criterion   66.60997\n```\n:::\n\n```{.r .cell-code}\nfit<-fitdist(srchscrlvce[srchscrlvce$Technique == \"Scroll\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi-squared statistic:  0.3816087 \nDegree of freedom of the Chi-squared distribution:  1 \nChi-squared p-value:  0.5367435 \n   the p-value may be wrong with some theoretical counts < 5  \nChi-squared table:\n     obscounts theocounts\n<= 0 10.000000   9.931706\n<= 1  6.000000   6.952194\n> 1   4.000000   3.116100\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   45.53208\nBayesian Information Criterion   46.52781\n```\n:::\n\n```{.r .cell-code}\nfit<-fitdist(srchscrlvce[srchscrlvce$Technique == \"Voice\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi-squared statistic:  0.1611327 \nDegree of freedom of the Chi-squared distribution:  3 \nChi-squared p-value:  0.9836055 \n   the p-value may be wrong with some theoretical counts < 5  \nChi-squared table:\n     obscounts theocounts\n<= 3  5.000000   5.161546\n<= 4  4.000000   3.473739\n<= 5  3.000000   3.508476\n<= 6  3.000000   2.952967\n> 6   5.000000   4.903272\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   84.19266\nBayesian Information Criterion   85.18839\n```\n:::\n:::\n\nAll three of the above goodness of fit tests tell us that there is no evidence of deviation from a Poisson distribution. Since we are now convinced of the Poisson distribution for each of the three techniques, analyze the errors using Poisson regression.\n\nWe've been saying \"set sum-to-zero contrasts for the Anova call\" but what does that mean? Contrasts are linear combinations used in ANOVA. As Wikipedia defines it, a contrast is a linear combination $\\sum^t_{i=1}a_i\\theta_i$, where each $\\theta_i$ is a statistic and the $a_i$ values sum to zero. Typically, the $a_i$ values are $1$ and $-1$. A simple contrast represents a difference between means and is used in ANOVA. In R, they are invisible if you use Type I ANOVA, but have to be specified as follows if using a Type III ANOVA. The default `anova()` function is Type I but we're using Type III, available from the `Anova()` function in the `car` package.\n\nA minor detail is that we don't really need to use `Anova()` here instead of `anova()` because the study is balanced, meaning that it has the same number of observations in each condition. The only reason for using `Anova()` on this data is that it gives a better-looking output. The `anova()` function would just display the $\\chi^2$ statistic without the associated $p$-value.\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(srchscrlvce$Technique) <- \"contr.sum\"\n#. family parameter identifies both distribution and link fn\nm <- glm(Errors ~ Technique, data=srchscrlvce, family=poisson)\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Errors\n          LR Chisq Df Pr(>Chisq)    \nTechnique    74.93  2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nBecause the Analysis of Deviance table shows a significant $\\chi^2$ value and corresponding $p$-value, we are justified to\nconduct pairwise comparisons among levels of Technique.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(multcomp)\nsummary(glht(m, mcp(Technique=\"Tukey\")), test=adjusted(type=\"holm\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glm(formula = Errors ~ Technique, family = poisson, data = srchscrlvce)\n\nLinear Hypotheses:\n                     Estimate Std. Error z value Pr(>|z|)    \nSearch - Scroll == 0   1.2730     0.3024   4.210 5.11e-05 ***\nVoice - Scroll == 0    1.9761     0.2852   6.929 1.27e-11 ***\nVoice - Search == 0    0.7031     0.1729   4.066 5.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n```\n:::\n:::\n\nWe see from the table that all three differences are significant. We could have guessed this result from glancing at the boxplot above, but it is valuable to have statistical evidence that this is not a chance difference.\n\n\n\n## More experiments without normally distributed errors (More generalized linear models)\n\n### Preference between touchpads vs trackballs by non / disabled people and males / females\n\nThis study examines whether participants of either sex with or without a disability prefer touchpads or trackballs. Start by examining the data and determining how many participants are involved.\n\n::: {.cell}\n\n```{.r .cell-code}\ndps <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/deviceprefssex.csv\"))\ndps$Subject<-as.factor(dps$Subject)\ndps$Disability<-as.factor(dps$Disability)\ndps$Sex<-as.factor(dps$Sex)\ndps$Pref<-as.factor(dps$Pref)\nsummary(dps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject   Disability Sex           Pref   \n 1      : 1   0:18       F:15   touchpad :21  \n 2      : 1   1:12       M:15   trackball: 9  \n 3      : 1                                   \n 4      : 1                                   \n 5      : 1                                   \n 6      : 1                                   \n (Other):24                                   \n```\n:::\n:::\n\nUse binomial regression to examine Pref by Disability and Sex. Report the $p$-value of the interaction of Disability$\\times$Sex.\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(dps$Disability) <- \"contr.sum\"\ncontrasts(dps$Sex) <- \"contr.sum\"\nm<-glm(Pref ~ Disability*Sex, data=dps, family=binomial)\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n               LR Chisq Df Pr(>Chisq)   \nDisability      10.4437  1   0.001231 **\nSex              2.8269  1   0.092695 . \nDisability:Sex   0.6964  1   0.403997   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nNow use multinomial regression for the same task and report the corresponding $p$-value.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(nnet)\ncontrasts(dps$Disability) <- \"contr.sum\"\ncontrasts(dps$Sex) <- \"contr.sum\"\nm<-multinom(Pref~Disability*Sex, data=dps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  5 (4 variable)\ninitial  value 20.794415 \niter  10 value 13.023239\niter  20 value 13.010200\nfinal  value 13.010184 \nconverged\n```\n:::\n\n```{.r .cell-code}\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Pref\n               LR Chisq Df Pr(>Chisq)   \nDisability      10.4434  1   0.001231 **\nSex              2.8267  1   0.092710 . \nDisability:Sex   0.6961  1   0.404087   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nNow conduct post-hoc binomial tests for each Disability$\\times$Sex combination.\n\n::: {.cell}\n\n```{.r .cell-code}\nm0<-binom.test(sum(dps[dps$Sex == \"M\" & dps$Disability == \"0\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"M\" & dps$Disability == \"0\",]),p=1/2)\nm1<-binom.test(sum(dps[dps$Sex == \"M\" & dps$Disability == \"1\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"M\" & dps$Disability == \"1\",]),p=1/2)\n\nf0<-binom.test(sum(dps[dps$Sex == \"F\" & dps$Disability == \"0\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"F\" & dps$Disability == \"0\",]),p=1/2)\nf1<-binom.test(sum(dps[dps$Sex == \"F\" & dps$Disability == \"1\",]$Pref == \"touchpad\"),\n               nrow(dps[dps$Sex == \"F\" & dps$Disability == \"1\",]),p=1/2)\n\np.adjust(c(m0$p.value, m1$p.value, f0$p.value,f1$p.value), method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0625000 1.0000000 0.1962891 1.0000000\n```\n:::\n:::\n\n### Handwriting recognition speed between different tools and right-handed vs left-handed people\n\nThis study examined three handwriting recognizers, A, B, and C and participants who are either right-handed or left-handed. The response is the number of incorrectly recognized handwritten words out of every 100 handwritten words. Examine the data and tell how many participants were involved.\n\n::: {.cell}\n\n```{.r .cell-code}\nhw <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/hwreco.csv\"))\nhw$Subject<-factor(hw$Subject)\nhw$Recognizer<-factor(hw$Recognizer)\nhw$Hand<-factor(hw$Hand)\nsummary(hw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject   Recognizer    Hand        Errors     \n 1      : 1   A:17       Left :25   Min.   : 1.00  \n 2      : 1   B:17       Right:25   1st Qu.: 3.00  \n 3      : 1   C:16                  Median : 4.00  \n 4      : 1                         Mean   : 4.38  \n 5      : 1                         3rd Qu.: 6.00  \n 6      : 1                         Max.   :11.00  \n (Other):44                                        \n```\n:::\n:::\n\nCreate an interaction plot of Recognizer on the $x$-axis and Hand as the traces and tell how many times the traces cross.\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(hw,interaction.plot(Recognizer,Hand,Errors,\n\t\t\t      ylim=c(0,max(hw$Errors))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-124-1.png){width=672}\n:::\n:::\n\nTest whether the Errors of each Recognizer fit a Poisson distribution. First fit the Poisson distribution using `fitdist()`, then test the fit using `gofstat()`. The null hypothesis of this test is that the data do not deviate from a Poisson distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(fitdistrplus)\nfit<-fitdist(hw[hw$Recognizer == \"A\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi-squared statistic:  1.807852 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.4049767 \n   the p-value may be wrong with some theoretical counts < 5  \nChi-squared table:\n     obscounts theocounts\n<= 2  4.000000   3.627277\n<= 3  5.000000   3.168895\n<= 5  4.000000   6.072436\n> 5   4.000000   4.131392\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   75.86792\nBayesian Information Criterion   76.70113\n```\n:::\n\n```{.r .cell-code}\nfit<-fitdist(hw[hw$Recognizer == \"B\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi-squared statistic:  0.9192556 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.6315187 \n   the p-value may be wrong with some theoretical counts < 5  \nChi-squared table:\n     obscounts theocounts\n<= 3  5.000000   3.970124\n<= 5  4.000000   5.800601\n<= 6  3.000000   2.588830\n> 6   5.000000   4.640444\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   78.75600\nBayesian Information Criterion   79.58921\n```\n:::\n\n```{.r .cell-code}\nfit<-fitdist(hw[hw$Recognizer == \"C\",]$Errors,\n              \"pois\", discrete=TRUE)\ngofstat(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi-squared statistic:  0.3521272 \nDegree of freedom of the Chi-squared distribution:  2 \nChi-squared p-value:  0.8385647 \n   the p-value may be wrong with some theoretical counts < 5  \nChi-squared table:\n     obscounts theocounts\n<= 2  5.000000   4.600874\n<= 3  4.000000   3.347372\n<= 4  3.000000   3.085858\n> 4   4.000000   4.965897\n\nGoodness-of-fit criteria\n                               1-mle-pois\nAkaike's Information Criterion   70.89042\nBayesian Information Criterion   71.66301\n```\n:::\n:::\n\nNow use Poisson regression to examine Errors by Recommender and Hand. Report the $p$-value for the Recognizer$\\times$Hand interaction.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(car)\ncontrasts(hw$Recognizer) <- \"contr.sum\"\ncontrasts(hw$Hand) <- \"contr.sum\"\nm<-glm(Errors ~ Recognizer*Hand, data=hw, family=poisson)\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Errors\n                LR Chisq Df Pr(>Chisq)   \nRecognizer        4.8768  2   0.087299 . \nHand              3.1591  1   0.075504 . \nRecognizer:Hand  12.9682  2   0.001528 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nConduct planned comparisons between left and right errors for each recognizer. Using `glht()` and `lsm()` will give all comparisons and we only want three so don't correct for multiple comparisons automatically. That would overcorrect. Instead, extract the three relevant $p$-values manually and and use `p.adjust()` to correct for those.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(multcomp) # for glht\n#. library(lsmeans) # for lsm\nsummary(glht(m, lsm(pairwise ~ Recognizer * Hand)),\n        test=adjusted(type=\"none\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = Errors ~ Recognizer * Hand, family = poisson, data = hw)\n\nLinear Hypotheses:\n                         Estimate Std. Error z value Pr(>|z|)    \nA Left - B Left == 0   -8.938e-01  2.611e-01  -3.423 0.000619 ***\nA Left - C Left == 0   -2.231e-01  3.000e-01  -0.744 0.456990    \nA Left - A Right == 0  -8.183e-01  2.638e-01  -3.102 0.001925 ** \nA Left - B Right == 0  -5.306e-01  2.818e-01  -1.883 0.059702 .  \nA Left - C Right == 0  -5.306e-01  2.818e-01  -1.883 0.059702 .  \nB Left - C Left == 0    6.707e-01  2.412e-01   2.780 0.005428 ** \nB Left - A Right == 0   7.551e-02  1.944e-01   0.388 0.697704    \nB Left - B Right == 0   3.632e-01  2.182e-01   1.665 0.095955 .  \nB Left - C Right == 0   3.632e-01  2.182e-01   1.665 0.095955 .  \nC Left - A Right == 0  -5.952e-01  2.441e-01  -2.438 0.014779 *  \nC Left - B Right == 0  -3.075e-01  2.635e-01  -1.167 0.243171    \nC Left - C Right == 0  -3.075e-01  2.635e-01  -1.167 0.243171    \nA Right - B Right == 0  2.877e-01  2.214e-01   1.299 0.193822    \nA Right - C Right == 0  2.877e-01  2.214e-01   1.299 0.193822    \nB Right - C Right == 0  3.331e-16  2.425e-01   0.000 1.000000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n```\n:::\n\n```{.r .cell-code}\np.adjust(c(0.001925,0.095955,0.243171),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.005775 0.191910 0.243171\n```\n:::\n:::\n\nThe above analyses suggest that the error counts were Poisson-distributed.\nThe above analyses suggest that there was a significant Recognizer$\\times$Hand interaction.\nThe above analyses suggest that for recognizer A, there were significantly more errors for right-handed participants than for left-handed participants.\n\n### Ease of booking international or domestic flights on three different services\n\nThis study describes flight bookings using one of three services, Expedia, Orbitz, or Priceline. Each booking was either International or Domestic and the Ease of each interaction was recorded on a 7 point Likert scale where 7 was easiest. Examine the data and determine the number of participants in the study.\n\n::: {.cell}\n\n```{.r .cell-code}\nbf <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/bookflights.csv\"))\nbf$Subject<-factor(bf$Subject)\nbf$International<-factor(bf$International)\nbf$Website<-factor(bf$Website)\nbf$International<-factor(bf$International)\nbf$Ease<-as.ordered(bf$Ease)\nsummary(bf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject         Website    International Ease   \n 1      :  1   Expedia  :200   0:300         1: 87  \n 2      :  1   Orbitz   :200   1:300         2: 58  \n 3      :  1   Priceline:200                 3:108  \n 4      :  1                                 4:107  \n 5      :  1                                 5: 95  \n 6      :  1                                 6: 71  \n (Other):594                                 7: 74  \n```\n:::\n:::\n\nDraw an interaction plot with Website on the x-axis and International as the traces. Determine how many times the traces cross.\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(bf,interaction.plot(Website,International,as.numeric(Ease),\n\t\t\t      ylim=c(0,max(as.numeric(bf$Ease)))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-129-1.png){width=672}\n:::\n:::\n\nUse ordinal logistic regression to examine Ease by Website and International. Report the $p$-value of the Website main effect.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(MASS) # provides polr()\n#. library(car) # provides Anova()\n#. set sum-to-zero contrasts for the Anova call\ncontrasts(bf$Website) <- \"contr.sum\"\ncontrasts(bf$International) <- \"contr.sum\"\nm <- polr(Ease ~ Website*International, data=bf, Hess=TRUE)\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Ease\n                      LR Chisq Df Pr(>Chisq)    \nWebsite                  6.811  2    0.03319 *  \nInternational            0.668  1    0.41383    \nWebsite:International   90.590  2    < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nConduct three pairwise comparisons of Ease between domestic and international for each service. Report the largest adjusted $p$-value. Use the same technique as above where you extracted the relevant unadjusted $p$-values manually and used `p.adjust()` to adjust them.\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(as.glht(pairs(lsmeans(m, pairwise ~ Website * International))),\n        test=adjusted(type=\"none\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n                                                         Estimate Std. Error\nExpedia International0 - Orbitz International0 == 0       -2.1442     0.2619\nExpedia International0 - Priceline International0 == 0    -0.9351     0.2537\nExpedia International0 - Expedia International1 == 0      -1.6477     0.2570\nExpedia International0 - Orbitz International1 == 0       -0.3217     0.2490\nExpedia International0 - Priceline International1 == 0    -0.7563     0.2517\nOrbitz International0 - Priceline International0 == 0      1.2091     0.2555\nOrbitz International0 - Expedia International1 == 0        0.4965     0.2505\nOrbitz International0 - Orbitz International1 == 0         1.8225     0.2571\nOrbitz International0 - Priceline International1 == 0      1.3879     0.2546\nPriceline International0 - Expedia International1 == 0    -0.7126     0.2518\nPriceline International0 - Orbitz International1 == 0      0.6134     0.2497\nPriceline International0 - Priceline International1 == 0   0.1789     0.2501\nExpedia International1 - Orbitz International1 == 0        1.3260     0.2524\nExpedia International1 - Priceline International1 == 0     0.8914     0.2506\nOrbitz International1 - Priceline International1 == 0     -0.4345     0.2477\n                                                         z value Pr(>|z|)    \nExpedia International0 - Orbitz International0 == 0       -8.189 2.22e-16 ***\nExpedia International0 - Priceline International0 == 0    -3.686 0.000228 ***\nExpedia International0 - Expedia International1 == 0      -6.411 1.44e-10 ***\nExpedia International0 - Orbitz International1 == 0       -1.292 0.196380    \nExpedia International0 - Priceline International1 == 0    -3.004 0.002663 ** \nOrbitz International0 - Priceline International0 == 0      4.732 2.22e-06 ***\nOrbitz International0 - Expedia International1 == 0        1.982 0.047498 *  \nOrbitz International0 - Orbitz International1 == 0         7.089 1.35e-12 ***\nOrbitz International0 - Priceline International1 == 0      5.452 4.99e-08 ***\nPriceline International0 - Expedia International1 == 0    -2.830 0.004659 ** \nPriceline International0 - Orbitz International1 == 0      2.457 0.014023 *  \nPriceline International0 - Priceline International1 == 0   0.715 0.474476    \nExpedia International1 - Orbitz International1 == 0        5.254 1.49e-07 ***\nExpedia International1 - Priceline International1 == 0     3.557 0.000375 ***\nOrbitz International1 - Priceline International1 == 0     -1.754 0.079408 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n```\n:::\n\n```{.r .cell-code}\np.adjust(c(1.44e-10,1.35e-12,0.474476),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.88000e-10 4.05000e-12 4.74476e-01\n```\n:::\n:::\n\nThe above analyses indicate a significant main effect of Website on Ease.\nThe above analyses indicate a significant interaction between Website and International.\nExpedia was perceived as significantly easier for booking international flights than domestic flights.\nOrbitz, on the other hand, was perceived as significantly easier for booking domestic flights than international flights.\n\n\n\n## Same person using different tools (Within subjects studies)\n\n### Two search engines compared\n\n::: {.cell}\n\n```{.r .cell-code}\nws <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch2.csv\"))\n```\n:::\n\nHow many subjects took part in this study?\n\n::: {.cell}\n\n```{.r .cell-code}\nws$Subject<-factor(ws$Subject)\nws$Engine<-factor(ws$Engine)\nsummary(ws)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject      Engine       Order        Searches         Effort    \n 1      : 2   Bing  :30   Min.   :1.0   Min.   : 89.0   Min.   :1.00  \n 2      : 2   Google:30   1st Qu.:1.0   1st Qu.:135.8   1st Qu.:2.00  \n 3      : 2               Median :1.5   Median :156.5   Median :4.00  \n 4      : 2               Mean   :1.5   Mean   :156.9   Mean   :3.90  \n 5      : 2               3rd Qu.:2.0   3rd Qu.:175.2   3rd Qu.:5.25  \n 6      : 2               Max.   :2.0   Max.   :241.0   Max.   :7.00  \n (Other):48                                                           \n```\n:::\n\n```{.r .cell-code}\ntail(ws)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  <fct>   <fct>  <dbl>    <dbl>  <dbl>\n1 28      Google     2      131      4\n2 28      Bing       1      192      4\n3 29      Google     1      162      5\n4 29      Bing       2      163      3\n5 30      Google     2      146      5\n6 30      Bing       1      137      2\n```\n:::\n:::\n\nThirty subjects participated.\n\nWhat is the average number of searches for the engine with the largest average number of searches?\n\n::: {.cell}\n\n```{.r .cell-code}\nws |>\n  group_by(Engine) |>\n  summarize(avg=mean(Searches))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  Engine   avg\n  <fct>  <dbl>\n1 Bing    166.\n2 Google  148.\n```\n:::\n:::\nBing had 166 searches on average.\n\nWhat is the $p$-value (four digits) from a paired-samples $t$-test of order effect?\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(reshape2)\nws.wide.order <- dcast(ws,Subject ~ Order, value.var=\"Searches\")\ntst<-t.test(ws.wide.order$\"1\",ws.wide.order$\"2\",paired=TRUE,var.equal=TRUE)\ntst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  ws.wide.order$\"1\" and ws.wide.order$\"2\"\nt = 0.34273, df = 29, p-value = 0.7343\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -13.57786  19.04453\nsample estimates:\nmean difference \n       2.733333 \n```\n:::\n:::\nThe $p$-value is 0.7343\n\nWhat is the $t$-statistic (two digits) for a paired-samples $t$-test of Searches by Engine?\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(reshape2)\nws.wide.engine <- dcast(ws,Subject ~ Engine, value.var=\"Searches\")\ntst<-t.test(ws.wide.engine$\"Bing\",ws.wide.engine$\"Google\",paired=TRUE,var.equal=TRUE)\ntst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  ws.wide.engine$Bing and ws.wide.engine$Google\nt = 2.5021, df = 29, p-value = 0.01824\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  3.310917 32.955750\nsample estimates:\nmean difference \n       18.13333 \n```\n:::\n:::\nThe $t$-statistic is 2.50.\n\nWhat is the $p$-value (four digits) from a Wilcoxon signed-rank test on Effort?\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(coin)\nwilcoxsign_test(Effort~Engine|Subject,data=ws,distribution=\"exact\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n\t stratified by block\nZ = 0.68343, p-value = 0.5016\nalternative hypothesis: true mu is not equal to 0\n```\n:::\n:::\nThe $p$-value is 0.5016.\n\n### Same but with three search engines\n\n::: {.cell}\n\n```{.r .cell-code}\nws3 <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch3.csv\"))\n```\n:::\n\nHow many subjects took part in this study?\n::: {.cell}\n\n```{.r .cell-code}\nsummary(ws3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject        Engine              Order      Searches         Effort     \n Min.   : 1.0   Length:90          Min.   :1   Min.   : 92.0   Min.   :1.000  \n 1st Qu.: 8.0   Class :character   1st Qu.:1   1st Qu.:139.0   1st Qu.:3.000  \n Median :15.5   Mode  :character   Median :2   Median :161.0   Median :4.000  \n Mean   :15.5                      Mean   :2   Mean   :161.6   Mean   :4.256  \n 3rd Qu.:23.0                      3rd Qu.:3   3rd Qu.:181.8   3rd Qu.:6.000  \n Max.   :30.0                      Max.   :3   Max.   :236.0   Max.   :7.000  \n```\n:::\n\n```{.r .cell-code}\nws3$Subject<-as.factor(ws3$Subject)\nws3$Order<-as.factor(ws3$Order)\nws3$Engine<-as.factor(ws3$Engine)\ntail(ws3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  <fct>   <fct>  <fct>    <dbl>  <dbl>\n1 29      Google 3          157      5\n2 29      Bing   1          195      4\n3 29      Yahoo  2          182      5\n4 30      Google 3          152      1\n5 30      Bing   2          188      7\n6 30      Yahoo  1          131      3\n```\n:::\n:::\n\nAgain, thirty subjects participated.\n\nWhat is the average number of searches for the engine with the largest average number of searches?\n\n::: {.cell}\n\n```{.r .cell-code}\nplyr::ddply(ws3,~ Engine,summarize, Mean=mean(Searches))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Engine     Mean\n1   Bing 159.8333\n2 Google 152.6667\n3  Yahoo 172.4000\n```\n:::\n:::\nYahoo required 172.40 searches on average.\n\nFind Mauchly's $W$ criterion (four digits) as a value of violation of sphericity.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(ez)\nm = ezANOVA(dv=Searches, within=Order, wid=Subject, data=ws3)\nm$Mauchly\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Effect         W         p p<.05\n2  Order 0.9416469 0.4309561      \n```\n:::\n:::\nMauchly's $W = 0.9416$, indicating that there is no violation of sphericity.\n\nConduct the appropriate ANOVA and give the $p$-value of the $F$-test (four digits).\n\n::: {.cell}\n\n```{.r .cell-code}\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Effect DFn DFd        F        p p<.05       ges\n2  Order   2  58 1.159359 0.320849       0.0278629\n```\n:::\n:::\nThe relevant $p$-value is 0.3208.\n\nConduct a repeated measures ANOVA on Searches by Engine and give the Mauchly's $W$ criterion (four digits).\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(ez)\nm <- ezANOVA(dv=Searches, within=Engine, wid=Subject, data=ws3)\nm$Mauchly\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Effect         W         p p<.05\n2 Engine 0.9420316 0.4334278      \n```\n:::\n:::\nMauchly's $W = 0.9420$, indicating that there is no violation of sphericity.\n\nConduct the appropriate ANOVA and give the $p$-value of the $F$-test (four digits).\n\n::: {.cell}\n\n```{.r .cell-code}\nm$ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Effect DFn DFd        F          p p<.05        ges\n2 Engine   2  58 2.856182 0.06560302       0.06498641\n```\n:::\n:::\nThe relevant $p$-value is 0.0656.\n\nConduct post-hoc paired sample $t$-tests among levels of Engine, assuming equal variances and using \"holm\" to correct for multiple comparisons. What is the smallest $p$-value (four digits)?\n::: {.cell}\n\n```{.r .cell-code}\n#. library(reshape2)\nws3.wide.engine <- dcast(ws3,Subject~Engine,value.var=\"Searches\")\nbi.go<-t.test(ws3.wide.engine$Bing,ws3.wide.engine$Google,paired=TRUE,var.equal=TRUE)\nbi.ya<-t.test(ws3.wide.engine$Bing,ws3.wide.engine$Yahoo,paired=TRUE,var.equal=TRUE)\ngo.ya<-t.test(ws3.wide.engine$Google,ws3.wide.engine$Yahoo,paired=TRUE,var.equal=TRUE)\np.adjust(c(bi.go$p.value,bi.ya$p.value,go.ya$p.value),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.37497103 0.37497103 0.05066714\n```\n:::\n:::\nThe smallest $p$-value is 0.0507.\n\nConduct a Friedman (nonparametric) test on Effort. Find the $\\chi^2$ statistic (four digits).\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(coin)\nfriedman_test(Effort~Engine|Subject,data=ws3,distribution=\"asymptotic\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAsymptotic Friedman Test\n\ndata:  Effort by\n\t Engine (Bing, Google, Yahoo) \n\t stratified by Subject\nchi-squared = 8.0182, df = 2, p-value = 0.01815\n```\n:::\n:::\n$\\chi^2=8.0182$\n\nConduct post hoc pairwise Wilcoxon signed-rank tests on Effort by Engine with \"holm\" for multiple comparison correction. Give the smallest $p$-value (four digits).\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(reshape2)\nws3.wide.effort <- dcast(ws3,Subject~Engine,value.var=\"Effort\")\nbi.go<-wilcox.test(ws3.wide.effort$Bing,ws3.wide.effort$Google,paired=TRUE,exact=FALSE)\nbi.ya<-wilcox.test(ws3.wide.effort$Bing,ws3.wide.effort$Yahoo,paired=TRUE,exact=FALSE)\ngo.ya<-wilcox.test(ws3.wide.effort$Google,ws3.wide.effort$Yahoo,paired=TRUE,exact=FALSE)\np.adjust(c(bi.go$p.value,bi.ya$p.value,go.ya$p.value),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.69319567 0.03085190 0.04533852\n```\n:::\n:::\nThe smallest $p$-value is 0.0309.\n\n\n\n## Experiments with people in groups doing tasks with different tools (Mixed models)\n\nMixed models contain both fixed effects and random effects. Following are linear mixed models and generalized linear mixed models examples. Recall that linear models have normally distributed residuals while generalized linear models may have residuals following other distributions.\n\n### Searching to find facts and effort of searching (A linear mixed model)\n\nLoad `websearch3.csv`. It describes a test of the number of searches required to find out a hundred facts and the perceived effort of searching.\nHow many subjects participated?\n\n::: {.cell}\n\n```{.r .cell-code}\nws <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/websearch3.csv\"))\nws<-within(ws,Subject<-factor(Subject))\nws<-within(ws,Order<-factor(Order))\nws<-within(ws,Engine<-factor(Engine))\ntail(ws)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  Subject Engine Order Searches Effort\n  <fct>   <fct>  <fct>    <dbl>  <dbl>\n1 29      Google 3          157      5\n2 29      Bing   1          195      4\n3 29      Yahoo  2          182      5\n4 30      Google 3          152      1\n5 30      Bing   2          188      7\n6 30      Yahoo  1          131      3\n```\n:::\n\n```{.r .cell-code}\nsummary(ws)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject      Engine   Order     Searches         Effort     \n 1      : 3   Bing  :30   1:30   Min.   : 92.0   Min.   :1.000  \n 2      : 3   Google:30   2:30   1st Qu.:139.0   1st Qu.:3.000  \n 3      : 3   Yahoo :30   3:30   Median :161.0   Median :4.000  \n 4      : 3                      Mean   :161.6   Mean   :4.256  \n 5      : 3                      3rd Qu.:181.8   3rd Qu.:6.000  \n 6      : 3                      Max.   :236.0   Max.   :7.000  \n (Other):72                                                     \n```\n:::\n:::\n\nWhat was the average number of Search instances for each Engine?\n\n::: {.cell}\n\n```{.r .cell-code}\nws |>\n  group_by(Engine) |>\n  summarize(median=median(Searches),\n\t    avg=mean(Searches),\n\t    sd=sd(Searches))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 4\n  Engine median   avg    sd\n  <fct>   <dbl> <dbl> <dbl>\n1 Bing     162.  160.  30.6\n2 Google   152   153.  24.6\n3 Yahoo    170.  172.  37.8\n```\n:::\n\n```{.r .cell-code}\nplyr::ddply(ws,~Engine,summarize,avg=mean(Searches))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Engine      avg\n1   Bing 159.8333\n2 Google 152.6667\n3  Yahoo 172.4000\n```\n:::\n:::\n\nConduct a linear mixed model analysis of variance on Search by Engine and report the $p$-value.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4) # for lmer\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Matrix'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lme4'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:RVAideMemoire':\n\n    dummy\n```\n:::\n\n```{.r .cell-code}\nlibrary(lmerTest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lmerTest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:lme4':\n\n    lmer\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    step\n```\n:::\n\n```{.r .cell-code}\n#. library(car) # for Anova\ncontrasts(ws$Engine) <- \"contr.sum\"\nm <- lmer(Searches ~ Engine + (1|Subject), data=ws)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nAnova(m, type=3, test.statistic=\"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: Searches\n                    F Df Df.res  Pr(>F)    \n(Intercept) 2374.8089  1     29 < 2e-16 ***\nEngine         3.0234  2     58 0.05636 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nConduct simultaneous pairwise comparisons among all levels of Engine, despite the previous $p$-value. Report the adjusted(by Holm's sequential Bonferroni procedure) $p$-values.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(multcomp)\nsummary(glht(m, mcp(Engine=\"Tukey\")),\n\ttest=adjusted(type=\"holm\")) # Tukey means compare all pairs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = Searches ~ Engine + (1 | Subject), data = ws)\n\nLinear Hypotheses:\n                    Estimate Std. Error z value Pr(>|z|)  \nGoogle - Bing == 0    -7.167      8.124  -0.882   0.3777  \nYahoo - Bing == 0     12.567      8.124   1.547   0.2438  \nYahoo - Google == 0   19.733      8.124   2.429   0.0454 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n```\n:::\n:::\n\n### People judging social media posts after viewing clips (Another linear mixed model)\n\nThe file `socialvalue.csv` describes a study of people viewing a positive or negative film clip then going onto social media and judging the value (1 or 0) of the first hundred posts they see. The number of valued posts was recorded. Load the file and tell how many participated.\n\n::: {.cell}\n\n```{.r .cell-code}\nsv <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/socialvalue.csv\"))\nsv<-within(sv,Subject<-factor(Subject))\nsv<-within(sv,Clip<-factor(Clip))\nsv<-within(sv,Social<-factor(Social))\nsv<-within(sv,ClipOrder<-factor(ClipOrder))\nsv<-within(sv,SocialOrder<-factor(SocialOrder))\nsummary(sv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject         Clip    ClipOrder      Social   SocialOrder     Valued    \n 1      : 4   negative:32   1:32      Facebook:32   1:32        Min.   :13.0  \n 2      : 4   positive:32   2:32      Twitter :32   2:32        1st Qu.:52.0  \n 3      : 4                                                     Median :56.0  \n 4      : 4                                                     Mean   :57.3  \n 5      : 4                                                     3rd Qu.:67.0  \n 6      : 4                                                     Max.   :95.0  \n (Other):40                                                                   \n```\n:::\n\n```{.r .cell-code}\ntail(sv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 6\n  Subject Clip     ClipOrder Social   SocialOrder Valued\n  <fct>   <fct>    <fct>     <fct>    <fct>        <dbl>\n1 15      negative 2         Facebook 2               60\n2 15      negative 2         Twitter  1               62\n3 16      positive 2         Facebook 2               34\n4 16      positive 2         Twitter  1               61\n5 16      negative 1         Facebook 1               59\n6 16      negative 1         Twitter  2               70\n```\n:::\n:::\n\nHow many more posts were valued on Facebook than on Twitter after seeing a positive clip?\n\n::: {.cell}\n\n```{.r .cell-code}\nout<-plyr::ddply(sv,~Clip*Social,summarize,ValuedAvg=mean(Valued))\nout\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Clip   Social ValuedAvg\n1 negative Facebook   46.3125\n2 negative  Twitter   55.5625\n3 positive Facebook   68.7500\n4 positive  Twitter   58.5625\n```\n:::\n\n```{.r .cell-code}\n68.75-58.5625\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.1875\n```\n:::\n:::\n\nConduct a linear mixed model analysis of variance on Valued by Social and Clip. Report the $p$-value of the interaction effect.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(lme4) # for lmer\n#. library(lmerTest)\n#. library(car) # for Anova\ncontrasts(sv$Social) <- \"contr.sum\"\ncontrasts(sv$Clip) <- \"contr.sum\"\nm <- lmer(Valued ~ (Social * Clip) + (1|Subject), data=sv)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nAnova(m, type=3, test.statistic=\"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: Valued\n                   F Df Df.res    Pr(>F)    \n(Intercept) 839.2940  1     15 1.392e-14 ***\nSocial        0.0140  1     45  0.906195    \nClip         10.3391  1     45  0.002413 ** \nSocial:Clip   6.0369  1     45  0.017930 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nConduct planned pairwise comparisons of how the clips may have influenced judgments about the value of social media. Report whether the number of valued posts differed after seeing a positive versus negative clip.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(multcomp) # for glht\n#. library(emmeans) # for lsm\nsummary(glht(m, lsm(pairwise ~ Social * Clip)),test=adjusted(type=\"none\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNote: df set to 45\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nFit: lmer(formula = Valued ~ (Social * Clip) + (1 | Subject), data = sv)\n\nLinear Hypotheses:\n                                           Estimate Std. Error t value Pr(>|t|)\nFacebook negative - Twitter negative == 0    -9.250      5.594  -1.654 0.105175\nFacebook negative - Facebook positive == 0  -22.438      5.594  -4.011 0.000225\nFacebook negative - Twitter positive == 0   -12.250      5.594  -2.190 0.033759\nTwitter negative - Facebook positive == 0   -13.188      5.594  -2.357 0.022810\nTwitter negative - Twitter positive == 0     -3.000      5.594  -0.536 0.594397\nFacebook positive - Twitter positive == 0    10.188      5.594   1.821 0.075234\n                                              \nFacebook negative - Twitter negative == 0     \nFacebook negative - Facebook positive == 0 ***\nFacebook negative - Twitter positive == 0  *  \nTwitter negative - Facebook positive == 0  *  \nTwitter negative - Twitter positive == 0      \nFacebook positive - Twitter positive == 0  .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n```\n:::\n\n```{.r .cell-code}\np.adjust(c(0.00017,0.59374),method=\"holm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.00034 0.59374\n```\n:::\n:::\n\n### People watching teasers in different orders and judging (Yet another linear mixed model)\n\nThe file `teaser.csv` describes a study in which people watched teasers for different genres and reported whether they liked them. Load the file and tell the number of participants.\n\n::: {.cell}\n\n```{.r .cell-code}\nte <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/teaser.csv\"))\nte<-within(te,Subject<-factor(Subject))\nte<-within(te,Order<-factor(Order))\nte<-within(te,Teaser<-factor(Teaser))\ntail(te)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  Subject Teaser   Order Liked\n  <fct>   <fct>    <fct> <dbl>\n1 19      thriller 4         1\n2 20      action   3         1\n3 20      comedy   2         0\n4 20      horror   4         0\n5 20      romance  1         0\n6 20      thriller 5         1\n```\n:::\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nboxplot(Liked~Teaser,data=te)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-156-1.png){width=672}\n:::\n:::\n\nInvestigate order effects.\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(te$Order) <- \"contr.sum\"\nm <- glmer(Liked ~ Order + (1|Subject), data=te, family=binomial, nAGQ=0)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Liked\n             Chisq Df Pr(>Chisq)\n(Intercept) 1.0392  1     0.3080\nOrder       3.9205  4     0.4169\n```\n:::\n:::\n\nConduct a linear mixed model analysis of variance.\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(te$Teaser) <- \"contr.sum\"\nm <- glmer(Liked ~ Teaser + (1|Subject), data=te, family=binomial, nAGQ=0)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Liked\n             Chisq Df Pr(>Chisq)    \n(Intercept)  1.209  1     0.2715    \nTeaser      26.695  4  2.291e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(multcomp)\nsummary(glht(m, mcp(Teaser=\"Tukey\")),\n\ttest=adjusted(type=\"holm\")) # Tukey means compare all pairs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glmer(formula = Liked ~ Teaser + (1 | Subject), data = te, family = binomial, \n    nAGQ = 0)\n\nLinear Hypotheses:\n                        Estimate Std. Error z value Pr(>|z|)    \ncomedy - action == 0     -3.7917     1.1361  -3.337 0.006763 ** \nhorror - action == 0     -5.1417     1.2681  -4.054 0.000502 ***\nromance - action == 0    -2.5390     1.1229  -2.261 0.118786    \nthriller - action == 0   -1.5581     1.1684  -1.334 0.389097    \nhorror - comedy == 0     -1.3499     0.8909  -1.515 0.389097    \nromance - comedy == 0     1.2528     0.6682   1.875 0.243191    \nthriller - comedy == 0    2.2336     0.7420   3.010 0.018279 *  \nromance - horror == 0     2.6027     0.8740   2.978 0.018279 *  \nthriller - horror == 0    3.5835     0.9317   3.846 0.001080 ** \nthriller - romance == 0   0.9808     0.7217   1.359 0.389097    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n```\n:::\n:::\n\n### Finding number of unique words used in posts by males and females (A generalized linear mixed model)\n\nThe file `vocab.csv` describes a study in which 50 posts by males and females were analyzed for the number of unique words used. Load the file and tell the number of participants.\n\n::: {.cell}\n\n```{.r .cell-code}\nvo <- read_csv(paste0(Sys.getenv(\"STATS_DATA_DIR\"),\"/vocab.csv\"))\nvo<-within(vo,Subject<-factor(Subject))\nvo<-within(vo,Sex<-factor(Sex))\nvo<-within(vo,Order<-factor(Order))\nvo<-within(vo,Social<-factor(Social))\ntail(vo)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  Subject Sex   Social   Order Vocab\n  <fct>   <fct> <fct>    <fct> <dbl>\n1 29      M     Facebook 3        46\n2 29      M     Twitter  1        38\n3 29      M     Gplus    2        22\n4 30      F     Facebook 3       103\n5 30      F     Twitter  2        97\n6 30      F     Gplus    1        92\n```\n:::\n:::\n\nCreate an interaction plot and see how often the lines cross.\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nwith(vo,interaction.plot(Social,Sex,Vocab,ylim=c(0,max(vo$Vocab))))\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-161-1.png){width=672}\n:::\n:::\n\nPerform Kolmogorov-Smirnov goodness-of-fit tests on Vocab for each level of Social using exponential distributions.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(MASS)\nfit = fitdistr(vo[vo$Social == \"Facebook\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Facebook\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in ks.test.default(vo[vo$Social == \"Facebook\", ]$Vocab, \"pexp\", : ties\nshould not be present for the Kolmogorov-Smirnov test\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Facebook\", ]$Vocab\nD = 0.17655, p-value = 0.2734\nalternative hypothesis: two-sided\n```\n:::\n\n```{.r .cell-code}\nfit = fitdistr(vo[vo$Social == \"Twitter\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Twitter\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in ks.test.default(vo[vo$Social == \"Twitter\", ]$Vocab, \"pexp\", rate =\nfit[1], : ties should not be present for the Kolmogorov-Smirnov test\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Twitter\", ]$Vocab\nD = 0.099912, p-value = 0.8966\nalternative hypothesis: two-sided\n```\n:::\n\n```{.r .cell-code}\nfit = fitdistr(vo[vo$Social == \"Gplus\",]$Vocab, \"exponential\")$estimate\nks.test(vo[vo$Social == \"Gplus\",]$Vocab, \"pexp\", rate=fit[1], exact=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  vo[vo$Social == \"Gplus\", ]$Vocab\nD = 0.14461, p-value = 0.5111\nalternative hypothesis: two-sided\n```\n:::\n:::\n\nUse a generallized linear mixed model to conduct a test of order effects on Vocab.\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(vo$Sex) <- \"contr.sum\"\ncontrasts(vo$Order) <- \"contr.sum\"\nm <- glmer(Vocab ~ Sex*Order + (1|Subject), data=vo, family=Gamma(link=\"log\"))\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Vocab\n                Chisq Df Pr(>Chisq)    \n(Intercept) 1179.0839  1     <2e-16 ***\nSex            1.3687  1     0.2420    \nOrder          0.7001  2     0.7047    \nSex:Order      2.0655  2     0.3560    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nConduct a test of Vocab by Sex and Social using a generalized linear mixed model.\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(vo$Sex) <- \"contr.sum\"\ncontrasts(vo$Social) <- \"contr.sum\"\nm = glmer(Vocab ~ Sex*Social + (1|Subject), data=vo, family=Gamma(link=\"log\"))\nAnova(m, type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III Wald chisquare tests)\n\nResponse: Vocab\n                Chisq Df Pr(>Chisq)    \n(Intercept) 1172.6022  1  < 2.2e-16 ***\nSex            0.7925  1     0.3733    \nSocial        26.2075  2  2.038e-06 ***\nSex:Social     0.3470  2     0.8407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nPerform post hoc pairwise comparisons among levels of Social adjusted with Holm's sequential Bonferroni procedure.\n\n::: {.cell}\n\n```{.r .cell-code}\n#. library(multcomp)\nsummary(glht(m, mcp(Social=\"Tukey\")),\n\ttest=adjusted(type=\"holm\")) # Tukey means compare all pairs\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in mcp2matrix(model, linfct = linfct): covariate interactions found --\ndefault contrast might be inappropriate\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: glmer(formula = Vocab ~ Sex * Social + (1 | Subject), data = vo, \n    family = Gamma(link = \"log\"))\n\nLinear Hypotheses:\n                        Estimate Std. Error z value Pr(>|z|)    \nGplus - Facebook == 0     0.8676     0.2092   4.148 6.72e-05 ***\nTwitter - Facebook == 0  -0.1128     0.2026  -0.556    0.578    \nTwitter - Gplus == 0     -0.9803     0.2071  -4.734 6.60e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n```\n:::\n:::\n\n### Judging search effort among different search engines (Another generalized linear mixed model)\n\nRecode Effort from `websearch3.csv` as an ordinal response.\n\n::: {.cell}\n\n```{.r .cell-code}\nws<-within(ws,Effort<-factor(Effort))\nws<-within(ws,Effort<-ordered(Effort))\nsummary(ws)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Subject      Engine   Order     Searches     Effort\n 1      : 3   Bing  :30   1:30   Min.   : 92.0   1: 6  \n 2      : 3   Google:30   2:30   1st Qu.:139.0   2: 8  \n 3      : 3   Yahoo :30   3:30   Median :161.0   3:19  \n 4      : 3                      Mean   :161.6   4:16  \n 5      : 3                      3rd Qu.:181.8   5:16  \n 6      : 3                      Max.   :236.0   6:15  \n (Other):72                                      7:10  \n```\n:::\n:::\n\nConduct an ordinal logistic regression to determine Effort by Engine, using a generalized linear mixed model.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ordinal) # provides clmm\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ordinal'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n\n```{.r .cell-code}\n#. library(RVAideMemoire) # provides Anova.clmm\nws2<-data.frame(ws)\nm<-clmm(Effort~Engine + (1|Subject),data=ws2)\nAnova.clmm(m,type=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Effort\n       LR Chisq Df Pr(>Chisq)  \nEngine    8.102  2     0.0174 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nPerform pairwise comparisons of Engine on Effort.\n\n::: {.cell}\n\n```{.r .cell-code}\npar(pin=c(2.75,1.25),cex=0.5)\nplot(as.numeric(Effort) ~ Engine,data=ws2)\n```\n\n::: {.cell-output-display}\n![](week13_files/figure-html/unnamed-chunk-168-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#. library(lme4)\n#. library(multcomp)\nm <- lmer(as.numeric(Effort)~Engine + (1|Subject), data=ws2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nsummary(glht(m,mcp(Engine=\"Tukey\")),test=adjusted(type=\"holm\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = as.numeric(Effort) ~ Engine + (1 | Subject), data = ws2)\n\nLinear Hypotheses:\n                    Estimate Std. Error z value Pr(>|z|)  \nGoogle - Bing == 0  -0.03333    0.42899  -0.078   0.9381  \nYahoo - Bing == 0    1.10000    0.42899   2.564   0.0247 *\nYahoo - Google == 0  1.13333    0.42899   2.642   0.0247 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n```\n:::\n:::\n\n\n\n\n<!--\n\nHow many people prefer website A over website B?\nHow many people prefer website A, website B, or website C?\nHow many males vs females prefer website A over website B?\nHow many males vs females prefer website A, website B, or website C?\nOn average, how many pages of website A vs B did people view?\nOn average, how long did people take to complete a task based on which of two tools they used?\nOn average, how long did people take to complete a task based on which of three tools they used?\nOn average, how long did people take to complete a task based on which of two tools they used (another pair of tools)?\nOn average, how many words per minute did people achieve writing with three different text entry systems?\nOn average, how long does it take to find contacts depending on whether you search or scroll?\nWhich takes more effort, searching or scrolling for contacts?\nWhich is more error-prone, searching or scrolling for contacts?\n-->\n",
    "supporting": [
      "week13_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}